{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers - Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as torch_data\n",
    "import math\n",
    "import copy\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Embeddings\n",
    "\n",
    "\n",
    "In essence, InputEmbeddings is the first step in a Transformer model that converts input data (like words in a sentence) into a format that the neural network can understand and process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Embedding: The core of this class is the nn.Embedding layer from PyTorch. This layer is responsible for representing each unique word (or token) in your vocabulary as a dense vector. These vectors capture semantic information about the words, making them suitable for processing by the Transformer.\n",
    "\n",
    "* Model Dimension: The model_dim parameter specifies the size of the embedding vectors. This dimension dictates how much information about each word is encoded in the embedding. A larger model dimension allows for more complex representations but also increases computational requirements.\n",
    "\n",
    "* Vocabulary Size: The vocab_size parameter determines how many unique words (or tokens) your model needs to represent. Each word in your vocabulary gets its own unique embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn as nn\n",
    "\n",
    "class InputEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    This class defines the input embedding layer for a neural network. \n",
    "    It converts input words (represented as indices) into dense vector representations (embeddings).\n",
    "    \"\"\"\n",
    "    def __init__(self, model_dim: int, vocab_size: int) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the InputEmbeddings module.\n",
    "\n",
    "        Args:\n",
    "            model_dim (int): The dimensionality of the embedding vectors.\n",
    "            vocab_size (int): The size of the vocabulary.\n",
    "        \"\"\"\n",
    "        super().__init__() # Calls Parent's constructor , i.e. nn.Module\n",
    "        self.model_dim = model_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size,embedding_dim=model_dim)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        Performs the forward pass of the module.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor representing a sequence of word indices.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Embedded representation of the input sequence.\n",
    "        \"\"\"\n",
    "        # This is the core function of the class, defining how the module processes input data.\n",
    "        # It takes an input tensor x, representing a sequence of words (usually indices into the vocabulary).\n",
    "        # It uses the embedding layer to lookup the corresponding embedding vector for each word in x.\n",
    "        # Scaling: The result is then multiplied by the square root of model_dim. \n",
    "        # NOTE : It helps with numerical stability and prevents the gradients from vanishing or exploding during training.\n",
    "        \n",
    "        return self.embedding(x) * math.sqrt(self.model_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The cat sat on the mat.\"\n",
    "\n",
    "\"\"\"\n",
    "First, you'd convert this sentence into a sequence of integers, \n",
    "representing each word's index in your vocabulary. \n",
    "Let's say:\n",
    "\"\"\"\n",
    "\n",
    "sentence_indices = [1, 2, 3, 4, 1, 5]  \n",
    "\n",
    "embeddings = InputEmbeddings(model_dim=512, vocab_size=10000)  # Example values\n",
    "# PyTorch automatically invokes the forward() method behind the scenes.\n",
    "embedded_sentence = embeddings(torch.tensor(sentence_indices)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 512])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_sentence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 41.9009,   8.8849,  -6.2731,  ...,  -9.9132, -21.3264, -11.4548],\n",
       "        [ 26.8885, -26.3593,  27.7979,  ..., -32.8299,  20.4210,   0.2708],\n",
       "        [-11.2794,  52.2548,  -4.8354,  ...,  -2.5915,  -2.3917, -17.4102],\n",
       "        [ 14.0425, -38.6314,  26.4700,  ...,  27.4415,  18.1834, -14.9053],\n",
       "        [ 41.9009,   8.8849,  -6.2731,  ...,  -9.9132, -21.3264, -11.4548],\n",
       "        [  5.6508,  13.5732,  -9.1930,  ...,  10.7587,  -7.4208, -26.4575]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summary\n",
    "The InputEmbeddings class effectively converts your input data from raw words or indices into dense vectors that the Transformer model can understand and manipulate. \n",
    "These vectors are crucial for the Transformer to learn relationships between words, understand the context of sentences, and ultimately perform tasks like translation, text generation, or question answering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <b>Importance</b>\n",
    "Positional encoding is essential for Transformers to understand the order of words in a sequence. Without it, the model wouldn't be able to differentiate between \"The cat sat on the mat\" and \"Mat the on sat cat the.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png][def]\n",
    "\n",
    "[def]: ./image.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where:\n",
    "\n",
    "- pos is the position of the word in the sequence.\n",
    "- i is the dimension index.\n",
    "- d_model is the size of the word embeddings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, model_dim:int, seq_len: int,dropout:float) -> None:\n",
    "        super().__init__()\n",
    "        self.model_dim = model_dim\n",
    "        self.seq_length = seq_len\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        #create a matrix of shape (seq_len, model_dim)\n",
    "        positional_encoder = torch.zeros(seq_len,model_dim)\n",
    "\n",
    "        # create a vector of shape(seq_len)\n",
    "        position_vector = torch.arange(0,seq_len,dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "        # create a vector of shape(d_model)\n",
    "        div_term = torch.exp(torch.arange(0,model_dim,2).float() * (-math.log(10000.0)/model_dim))\n",
    "\n",
    "        # Apply sine to even indexes\n",
    "        positional_encoder[:,0::2] = torch.sin(position_vector * div_term)\n",
    "\n",
    "        # Apply cosine to odd indexes\n",
    "        positional_encoder[:,1::2] = torch.cos(position_vector * div_term)\n",
    "\n",
    "        ## Add a batch dimension to positional encoding\n",
    "        positional_encoder = positional_encoder.unsqueeze(0)\n",
    "\n",
    "        # Register the encoding as buffer\n",
    "        self.register_buffer('pe',positional_encoder)\n",
    "\n",
    "    \"\"\"\n",
    "    The forward method takes the output of the input embeddings (x) as input.\n",
    "    It adds the positional encoding matrix (sliced to the length of the input sequence) \n",
    "    to the input embeddings.\n",
    "    The resulting tensor is passed through the dropout layer to prevent overfitting.\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = x +  (self.pe[:,:x.shape[1],:]).requires_grad_(False)\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Indices: tensor([[632, 213, 745, 631, 616, 670, 124, 128, 333, 359, 439, 997, 618, 839,\n",
      "         600, 316, 935, 981, 606, 530]])\n",
      "Embedded Input Shape: torch.Size([1, 20, 256])\n",
      "Encoded Input Shape: torch.Size([1, 20, 256])\n",
      "Encoded Input (First Few Elements):\n",
      " tensor([[ -5.8089,  29.9849,  -0.0000,  ...,  -4.6928,  -1.7949,  -0.0000],\n",
      "        [ -3.8191,  -3.2951,  -6.8730,  ...,  39.1492,  14.5779,  21.3420],\n",
      "        [  8.8207,  13.9228,  20.5074,  ...,   0.1306,  38.0097,   8.8268],\n",
      "        [ -0.0000, -24.6249,  13.2669,  ...,  -0.1755,  -6.7886,  20.4346],\n",
      "        [  6.7219,  18.7789,  15.6983,  ..., -14.4984,  10.3004,   5.5087]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Set parameters\n",
    "vocab_size = 1000  # Example vocabulary size\n",
    "model_dim = 256    # Embedding dimension\n",
    "seq_length = 20  # Maximum sequence length\n",
    "dropout = 0.1    # Dropout probability\n",
    "\n",
    "# Create instances of the modules\n",
    "input_embeddings = InputEmbeddings(model_dim, vocab_size)\n",
    "positional_encoding = PositionalEncoding(model_dim, seq_length, dropout)\n",
    "\n",
    "# Sample input (random indices into the vocabulary)\n",
    "input_indices = torch.randint(0, vocab_size, (1, seq_length))\n",
    "\n",
    "# Perform the embedding and positional encoding\n",
    "embedded_input = input_embeddings(input_indices)\n",
    "encoded_input = positional_encoding(embedded_input)\n",
    "\n",
    "# Print the results\n",
    "print(\"Input Indices:\", input_indices)\n",
    "print(\"Embedded Input Shape:\", embedded_input.shape)\n",
    "print(\"Encoded Input Shape:\", encoded_input.shape)\n",
    "\n",
    "# You can also visualize the encoded input to see how positional information is added\n",
    "print(\"Encoded Input (First Few Elements):\\n\", encoded_input[0, :5, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiHead-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1,..., \\text{head}_h) W^O\n",
    "$$\n",
    "$$\n",
    "\\text{where}, \\quad \\text{head}_i = \\text{Attention}(Q W_Q^i, K W_K^i, V W_V^i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaled Dot-Product Attention\n",
    "\n",
    "\\begin{align}\n",
    "\\text Attention(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, model_dim:int, h:int, dropout:float) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding Model Size\n",
    "        self.model_dim = model_dim\n",
    "        # Number of heads\n",
    "        self.h = h\n",
    "\n",
    "        # Make sure d_model is divisible by h\n",
    "        assert model_dim % h == 0, \"d_model is not divisible by h\"\n",
    "\n",
    "        # Initializing dimensions\n",
    "        ## Dimension of vector seen by each head\n",
    "        self.d_k = self.model_dim // h\n",
    "        # Wq, Wk, Wv, Wo - Linear layers for query, key, value and output transformations\n",
    "        self.w_q = nn.Linear(model_dim,model_dim,bias=False)\n",
    "        self.w_k = nn.Linear(model_dim,model_dim,bias=False)\n",
    "        self.w_v = nn.Linear(model_dim,model_dim,bias=False)\n",
    "        self.w_o = nn.Linear(model_dim,model_dim,bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(query,key,value,mask, dropout : nn.Dropout):\n",
    "        \"\"\"\n",
    "        Scaled Dot-Product Attention\n",
    "\n",
    "        Args:\n",
    "            query (torch.Tensor): Query tensor of shape (batch_size, seq_len, model_dim).\n",
    "            key (torch.Tensor): Key tensor of shape (batch_size, seq_len, model_dim).\n",
    "            value (torch.Tensor): Value tensor of shape (batch_size, seq_len, model_dim).\n",
    "            mask (torch.Tensor): Mask tensor of shape (batch_size, seq_len), where 1 indicates a valid position and 0 indicates a masked position.\n",
    "            dropout (nn.Dropout): _description_\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        # Calculate dimension of the query vector\n",
    "        d_k = query.shape[-1]\n",
    "        # Perform scaled dot-product attention here... \n",
    "        # (Calculations for attention weights and output)\n",
    "        # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len) \n",
    "        attention_scores = (query @ key.transpose(-2,-1))/ math.sqrt(d_k)\n",
    "\n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            # ref: https://pytorch.org/docs/stable/generated/torch.Tensor.masked_fill_.html\n",
    "            # Set the attention scores corresponding to masked positions to a very low value (-1e9).\n",
    "            # This ensures that the model does not attend to the masked positions.\n",
    "            attention_scores.masked_fill_(mask==0,-1e9)\n",
    "        \n",
    "        # Normalize attention scores using softmax\n",
    "        attention_scores = attention_scores.softmax(dim=-1)\n",
    "\n",
    "        # Apply dropout to attention scores if dropout is not None\n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "        \n",
    "        # Multiply attention scores by the value matrix to get the output\n",
    "        return (attention_scores @ value) , attention_scores\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "        \"\"\"\n",
    "        Forward pass of the Multi-Head Attention layer.\n",
    "\n",
    "        Args:\n",
    "            q (torch.Tensor): Query tensor of shape (batch_size, seq_len, model_dim).\n",
    "            k (torch.Tensor): Key tensor of shape (batch_size, seq_len, model_dim).\n",
    "            v (torch.Tensor): Value tensor of shape (batch_size, seq_len, model_dim).\n",
    "            mask (torch.Tensor): Mask tensor of shape (batch_size, seq_len), where 1 indicates a valid position and 0 indicates a masked position.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, seq_len, model_dim).\n",
    "        \"\"\"\n",
    "\n",
    "        # Apply linear transformations to query, key, and value tensors\n",
    "        query = self.w_q(q)\n",
    "        key = self.w_k(k)\n",
    "        value = self.w_v(v)\n",
    "\n",
    "        # Reshape and transpose query, key, and value tensors to prepare for multi-head attention\n",
    "        query = query.view(query.shape[0],query.shape[1],self.h,self.d_k).transpose(1,2)\n",
    "        key = key.view(key.shape[0],key.shape[1],self.h,self.d_k).transpose(1,2)\n",
    "        value = value.view(value.shape[0],value.shape[1],self.h,self.d_k).transpose(1,2)\n",
    "        \n",
    "        # Calculate attention\n",
    "        # The static method 'attention' is used here to perform the scaled dot-product attention\n",
    "        # The returned value 'x' is the output of the attention mechanism\n",
    "        # 'self.attention_scores' stores the attention weights for later use\n",
    "        x, self.attention_scores = MultiHeadAttention.attention(query,key, value, mask, self.dropout)\n",
    "\n",
    "        # Combine all the heads together\n",
    "        # The output 'x' is reshaped and transposed to combine the outputs of all heads\n",
    "        # The output 'x' now has the shape (batch_size, seq_len, model_dim)\n",
    "        x = x.transpose(1,2).contiguous().view(x.shape[0],-1, self.h * self.d_k)\n",
    "\n",
    "        # Multiply by Wo\n",
    "        # The final output is obtained by applying a linear transformation to the combined output 'x'\n",
    "        return self.w_o(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Output Shape: torch.Size([1, 20, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.2879,  5.5303, -0.8271,  ..., -3.5598, -0.0692,  2.5537],\n",
       "         [-0.4942, -3.5450, -4.1114,  ...,  6.4556, -3.5498, -7.0402],\n",
       "         [-4.7805, -0.1277,  2.7567,  ..., -3.5298,  0.5763, -1.8399],\n",
       "         ...,\n",
       "         [-7.3817,  0.5795,  8.3423,  ...,  0.8757, -5.3898,  7.5155],\n",
       "         [-8.9999, -2.4231, -0.7978,  ...,  0.7756,  2.1163,  0.4824],\n",
       "         [ 1.5358, -5.0703, -4.6353,  ...,  1.4985,  3.1268, 13.1597]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set parameters\n",
    "vocab_size = 1000\n",
    "model_dim = 256\n",
    "seq_length = 20\n",
    "h = 8      # Number of attention heads\n",
    "dropout = 0.1\n",
    "\n",
    "# Create instances of the modules\n",
    "input_embeddings = InputEmbeddings(model_dim, vocab_size)\n",
    "positional_encoding = PositionalEncoding(model_dim, seq_length, dropout)\n",
    "multihead_attention = MultiHeadAttention(model_dim, h, dropout)\n",
    "\n",
    "# Sample input (random indices)\n",
    "input_indices = torch.randint(0, vocab_size, (1, seq_length))\n",
    "\n",
    "# Embed and encode the input\n",
    "embedded_input = input_embeddings(input_indices)\n",
    "encoded_input = positional_encoding(embedded_input)\n",
    "\n",
    "# Perform multi-head attention (using encoded input as query, key, and value)\n",
    "attention_output = multihead_attention(encoded_input, encoded_input, encoded_input, None)  # No mask in this example\n",
    "\n",
    "print(\"Attention Output Shape:\", attention_output.shape)  # (batch_size, seq_len, model_dim)\n",
    "attention_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LayerNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int , eps: float= 10 ** -6) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the LayerNormalization module.\n",
    "\n",
    "        Args:\n",
    "            features (int): The number of features in the input tensor.\n",
    "            eps (float, optional): A small value added to the denominator to prevent division by zero. Defaults to 10^-6.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        # alpha and bias are learnable parameters\n",
    "        self.alpha = nn.Parameter(torch.ones(features))\n",
    "        self.bias = nn.Parameter(torch.ones(features))\n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        Applies LayerNormalization to the input tensor.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor with shape (batch_size, seq_len, hidden_size).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The normalized output tensor with the same shape as the input.\n",
    "        \"\"\"\n",
    "        #x :( batch_size, seq_len, hidden_size)\n",
    "        \n",
    "        mean = x.mean(dim = -1, keepdim = True) #(batch,seq_len, 1)\n",
    "        std = x.std(dim = -1 , keepdim = True)\n",
    "\n",
    "        return self.alpha * (x - mean) / (std + self.eps) +  self.bias\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FeedForward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    A feedforward neural network with two linear layers and ReLU activation.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_dim:int, d_ff:int, dropout:float) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the FeedForward module.\n",
    "\n",
    "        Args:\n",
    "            model_dim (int): The dimension of the input and output.\n",
    "            d_ff (int): The dimension of the hidden layer.\n",
    "            dropout (float): The dropout probability.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # w1 and b1\n",
    "        self.linear_1 = nn.Linear(model_dim,d_ff) # First linear layer\n",
    "        self.dropout = nn.Dropout(dropout) # Dropout layer\n",
    "        # w2 and b2\n",
    "        self.linear_2 = nn.Linear(d_ff,model_dim) # Second linear layer\n",
    "    \n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        Performs the forward pass of the network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor.\n",
    "        \"\"\"\n",
    "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer : ResidualConnection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements a residual connection with dropout and layer normalization.\n",
    "\n",
    "    Args:\n",
    "        features (int): The number of features in the input tensor.\n",
    "        dropout (float): The dropout probability.\n",
    "    \"\"\"\n",
    "    def __init__(self, features: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        # Initialize dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Initialize layer normalization\n",
    "        self.norm  = LayerNormalization(features=features)\n",
    "    \n",
    "    def forward(self, x, sublayer):\n",
    "        \"\"\"\n",
    "        Performs the forward pass of the residual connection.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor.\n",
    "            sublayer (nn.Module): The sublayer to apply to the input.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor.\n",
    "        \"\"\"\n",
    "        # Apply layer normalization to the input\n",
    "        normalized_x = self.norm(x)\n",
    "        # Apply the sublayer to the normalized input\n",
    "        sublayer_output = sublayer(normalized_x)\n",
    "        # Apply dropout to the sublayer output\n",
    "        dropped_output = self.dropout(sublayer_output)\n",
    "        # Add the original input to the dropped sublayer output\n",
    "        return x + dropped_output\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Represents a single encoder block in a Transformer architecture. \n",
    "\n",
    "    Args:\n",
    "        features (int): Number of features in the input and output of the block.\n",
    "        self_attention (MultiHeadAttention): The multi-head self-attention module.\n",
    "        feed_forward (FeedForward): The feed-forward neural network.\n",
    "        dropout (float): Dropout probability.\n",
    "    \"\"\"\n",
    "    def __init__(self, features: int, self_attention:MultiHeadAttention, feed_forward: FeedForward, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize the self-attention and feed-forward modules\n",
    "        self.self_attention = self_attention\n",
    "        self.feed_forward = feed_forward\n",
    "\n",
    "        # Create a list of residual connections, one for each sub-layer\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(features,dropout) for _ in range(2)])\n",
    "    \n",
    "    def forward(self, x, src_mask):\n",
    "        \"\"\"\n",
    "        Performs a forward pass through the encoder block.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, sequence_length, features).\n",
    "            src_mask (torch.Tensor): Mask for the source sequence, used in the self-attention layer.\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, sequence_length, features).\n",
    "        \"\"\"\n",
    "        # Apply self-attention with residual connection\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention(x,x,x,src_mask))\n",
    "\n",
    "        # Apply feed-forward network with residual connection\n",
    "        x = self.residual_connections[1](x, self.feed_forward)\n",
    "\n",
    "        # Return the output of the encoder block\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder module for the Transformer model.\n",
    "\n",
    "    Args:\n",
    "        features (int): The number of features in the input sequence.\n",
    "        layers (nn.ModuleList): A list of encoder layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, features: int, layers: nn.ModuleList ) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers  # Store the encoder layers\n",
    "        self.norm = LayerNormalization(features=features) # Initialize layer normalization\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        Forward pass of the encoder.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input sequence.\n",
    "            mask (torch.Tensor): The attention mask.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The encoded sequence.\n",
    "        \"\"\"\n",
    "        for layer in self.layers:  # Iterate through each encoder layer\n",
    "            x = layer(x, mask)  # Apply the layer to the input\n",
    "            \n",
    "        return self.norm(x)  # Apply layer normalization to the output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A single decoder block in the Transformer architecture.\n",
    "\n",
    "    Args:\n",
    "        features (int): Number of features in the input and output.\n",
    "        self_attention (MultiHeadAttention): Multi-head self-attention module.\n",
    "        cross_attention (MultiHeadAttention): Multi-head cross-attention module.\n",
    "        feed_forward (FeedForward): Feed-forward neural network module.\n",
    "        dropout (float): Dropout probability.\n",
    "    \"\"\"\n",
    "    def __init__(self, features: int, self_attention: MultiHeadAttention, cross_attention:MultiHeadAttention, feed_forward: FeedForward, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention = self_attention\n",
    "        self.cross_attention = cross_attention\n",
    "        self.feed_forward = feed_forward\n",
    "        # Create 3 residual connections for self-attention, cross-attention, and feed-forward\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(features,dropout) for _ in range(3)])\n",
    "    \n",
    "\n",
    "    def forward(self, x , encoder_output, src_mask, tgt_mask):\n",
    "        \"\"\"\n",
    "        Forward pass of the decoder block.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Decoder input, shape (batch_size, seq_len, features).\n",
    "            encoder_output (torch.Tensor): Encoder output, shape (batch_size, src_seq_len, features).\n",
    "            src_mask (torch.Tensor): Source mask, shape (batch_size, src_seq_len).\n",
    "            tgt_mask (torch.Tensor): Target mask, shape (batch_size, tgt_seq_len).\n",
    "        Returns:\n",
    "            torch.Tensor: Decoder output, shape (batch_size, seq_len, features).\n",
    "        \"\"\"\n",
    "        # Apply self-attention with residual connection\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention(x,x,x, tgt_mask))\n",
    "        # Apply cross-attention with residual connection\n",
    "        x = self.residual_connections[1](x, lambda x: self.cross_attention(x,encoder_output,encoder_output,src_mask))\n",
    "        # Apply feed-forward with residual connection\n",
    "        x = self.residual_connections[2](x, self.feed_forward)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    The decoder module for the Transformer model.\n",
    "\n",
    "    Args:\n",
    "        features (int): The number of features in the decoder.\n",
    "        layers (nn.ModuleList): A list of decoder layers.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, features: int , layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize the decoder layers\n",
    "        self.layers = layers\n",
    "        # Initialize the layer normalization layer\n",
    "        self.norm = LayerNormalization(features= features)\n",
    "\n",
    "    def forward(self,x, encoder_output, src_mask, tgt_mask):\n",
    "        \"\"\"\n",
    "        Forward pass of the decoder.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The decoder input.\n",
    "            encoder_output (torch.Tensor): The encoder output.\n",
    "            src_mask (torch.Tensor): The source mask.\n",
    "            tgt_mask (torch.Tensor): The target mask.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The decoder output.\n",
    "        \"\"\"\n",
    "        # Iterate over the decoder layers\n",
    "        for layer in self.layers:\n",
    "            # Apply the current layer to the input\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "        \n",
    "        # Apply layer normalization to the output\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projection Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    This class defines a projection layer that maps a hidden state representation \n",
    "    to a vocabulary space. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_dim: int, vocab_size: int) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the ProjectionLayer.\n",
    "\n",
    "        Args:\n",
    "            model_dim (int): The dimensionality of the hidden state.\n",
    "            vocab_size (int): The size of the vocabulary.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Define a linear layer to project the hidden state to the vocabulary size\n",
    "        self.proj = nn.Linear(model_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x) -> None:\n",
    "        \"\"\"\n",
    "        Performs the forward pass of the projection layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The hidden state representation.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The projected output in the vocabulary space.\n",
    "        \"\"\"\n",
    "        # Apply the linear projection to the input\n",
    "        return self.proj(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings,src_pos: PositionalEncoding, tgt_pos: PositionalEncoding,projection_layer:ProjectionLayer):\n",
    "        \"\"\"\n",
    "        Initializes the Transformer model.\n",
    "\n",
    "        Args:\n",
    "            encoder (Encoder): The encoder module.\n",
    "            decoder (Decoder): The decoder module.\n",
    "            src_embed (InputEmbeddings): The source embedding layer.\n",
    "            tgt_embed (InputEmbeddings): The target embedding layer.\n",
    "            src_pos (PositionalEncoding): The source positional encoding layer.\n",
    "            tgt_pos (PositionalEncoding): The target positional encoding layer.\n",
    "            projection_layer (ProjectionLayer): The projection layer for the final output.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.src_pos = src_pos\n",
    "        self.tgt_pos = tgt_pos\n",
    "        self.projection_layer = projection_layer\n",
    "    \n",
    "    def encode(self, src, src_mask):\n",
    "        \"\"\"\n",
    "        Encodes the source sequence.\n",
    "\n",
    "        Args:\n",
    "            src (torch.Tensor): The source sequence.\n",
    "            src_mask (torch.Tensor): The source mask.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The encoded source sequence.\n",
    "        \"\"\"\n",
    "        src = self.src_embed(src)\n",
    "        src = self.src_pos(src)\n",
    "        return self.encoder(src, src_mask)\n",
    "    \n",
    "    def decode(self, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt: torch.Tensor, tgt_mask: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Decodes the target sequence given the encoded source sequence.\n",
    "\n",
    "        Args:\n",
    "            encoder_output (torch.Tensor): The encoded source sequence.\n",
    "            src_mask (torch.Tensor): The source mask.\n",
    "            tgt (torch.Tensor): The target sequence.\n",
    "            tgt_mask (torch.Tensor): The target mask.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The decoded target sequence.\n",
    "        \"\"\"\n",
    "        tgt = self.tgt_embed(tgt)\n",
    "        tgt = self.tgt_pos(tgt)\n",
    "        return self.decoder(tgt,encoder_output,src_mask,tgt_mask)\n",
    "    \n",
    "    def project(self,x):\n",
    "        \"\"\"\n",
    "        Projects the output of the decoder to the target vocabulary.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The output of the decoder.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The projected output.\n",
    "        \"\"\"\n",
    "        return self.projection_layer(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer(src_vocab_size: int, tgt_vocab_size:int, src_seq_len:int, tgt_seq_len: int, model_dim: int = 512 , N: int = 6, h : int = 8, dropout: float= 0.1, d_ff: int = 2048):\n",
    "    # Create Embedding Layers\n",
    "    src_embed = InputEmbeddings(model_dim,vocab_size= src_vocab_size) # Embedding layer for source language\n",
    "    tgt_embed = InputEmbeddings(model_dim,vocab_size= tgt_vocab_size) # Embedding layer for target language\n",
    "\n",
    "    # Create Positional Encoding Layers\n",
    "    src_pos = PositionalEncoding(model_dim, seq_len = src_seq_len,dropout=dropout) # Positional encoding for source language\n",
    "    tgt_pos = PositionalEncoding(model_dim, seq_len = tgt_seq_len,dropout=dropout) # Positional encoding for target language\n",
    "\n",
    "    # Create encoder blocks\n",
    "    encoder_blocks = []\n",
    "    for _ in range(N): # Create N encoder blocks\n",
    "        encoder_self_attention_block = MultiHeadAttention(model_dim,h,dropout) # Multi-head self-attention block for encoder\n",
    "        feed_forward_block = FeedForward(model_dim,d_ff,dropout) # Feed-forward network for encoder\n",
    "        encoder_block = EncoderBlock(features=model_dim,self_attention=encoder_self_attention_block,feed_forward=feed_forward_block,dropout=dropout) # Combine attention and feedforward for an encoder block\n",
    "        encoder_blocks.append(encoder_block)\n",
    "\n",
    "    # Create decoder blocks\n",
    "    decoder_blocks = []\n",
    "    for _ in range(N): # Create N decoder blocks\n",
    "        decoder_self_attention_block = MultiHeadAttention(model_dim,h,dropout) # Multi-head self-attention block for decoder\n",
    "        decoder_cross_attention_block = MultiHeadAttention(model_dim,h, dropout) # Multi-head cross-attention block for decoder\n",
    "\n",
    "        feed_forward_block = FeedForward(model_dim,d_ff,dropout) # Feed-forward network for decoder\n",
    "        decoder_block = DecoderBlock(features=model_dim,self_attention=decoder_self_attention_block,cross_attention=decoder_cross_attention_block,feed_forward=feed_forward_block,dropout=dropout) # Combine attention and feedforward for a decoder block\n",
    "        decoder_blocks.append(decoder_block)\n",
    "        \n",
    "    \n",
    "    # Create the encoder and decoder block\n",
    "    encoder = Encoder(model_dim, nn.ModuleList(encoder_blocks)) # Combine all encoder blocks\n",
    "    decoder = Decoder(model_dim, nn.ModuleList(decoder_blocks)) # Combine all decoder blocks\n",
    "\n",
    "    # Create the projection layer\n",
    "    projection_layer = ProjectionLayer(model_dim=model_dim,vocab_size=tgt_vocab_size) # Layer to project decoder output to target language vocabulary\n",
    "\n",
    "    # Create the Transformer\n",
    "    transformer = Transformer(encoder,decoder,src_embed,tgt_embed,src_pos,tgt_pos,projection_layer) # Combine all components into the transformer\n",
    "\n",
    "    # Intialize the parameters , rather than starting randomly\n",
    "    for p in transformer.parameters(): \n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform(p) # Initialize parameters using Xavier uniform initialization\n",
    "    return transformer\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Transformer NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets tokenizers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nityavg/Labs/ML4Interviews/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: This is a test with some inner quotes.\n",
      "Text 2: This is another test with more inner quotes.\n",
      "Text 3: This has single and double quotes.\n"
     ]
    }
   ],
   "source": [
    "def remove_inner_quotes(text):\n",
    "  \"\"\"\n",
    "  Removes inner quotes (both single and double) if they form a pair.\n",
    "\n",
    "  Args:\n",
    "    text: The input string with potential inner quotes.\n",
    "\n",
    "  Returns:\n",
    "    The string with inner quotes removed, if present.\n",
    "  \"\"\"\n",
    "  quote_type = None\n",
    "  result = []\n",
    "  for char in text:\n",
    "    if char in (\"'\", '\"'):\n",
    "      if quote_type is None:\n",
    "        quote_type = char\n",
    "      elif char == quote_type:\n",
    "        quote_type = None\n",
    "      else:\n",
    "        result.append(char)\n",
    "    else:\n",
    "      result.append(char)\n",
    "  return \"\".join(result)\n",
    "\n",
    "# Example usage:\n",
    "text1 = \"This is a test 'with' some inner quotes.\"\n",
    "text2 = 'This is another test \"with\" more inner quotes.'\n",
    "text3 = \"This has 'single' and \\\"double\\\" quotes.\"\n",
    "\n",
    "print(f\"Text 1: {remove_inner_quotes(text1)}\")\n",
    "print(f\"Text 2: {remove_inner_quotes(text2)}\")\n",
    "print(f\"Text 3: {remove_inner_quotes(text3)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_trailing_newline(text):\n",
    "  \"\"\"\n",
    "  Removes trailing newline characters (\\n) from a string.\n",
    "\n",
    "  Args:\n",
    "    text: The input string.\n",
    "  Returns:\n",
    "    The string with trailing newlines removed.\n",
    "  \"\"\"\n",
    "  return text.rstrip('\\n').strip()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def preprocess(text: str):\n",
    "  \"\"\"\n",
    "  Preprocesses a string by removing inner quotes and trailing newlines.\n",
    "\n",
    "  Args:\n",
    "    text: The input string.\n",
    "\n",
    "  Returns:\n",
    "    The preprocessed string.\n",
    "  \"\"\"\n",
    "  text = remove_inner_quotes(text) # Remove inner quotes\n",
    "  text = remove_trailing_newline(text) # Remove trailing newlines\n",
    "  return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langdetect'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangdetect\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdetect_text_language\u001b[39m(text):\n\u001b[32m      4\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[33;03m  Detects the language of the given text using the langdetect library.\u001b[39;00m\n\u001b[32m      6\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m \u001b[33;03m    or 'Unknown' if no language could be confidently detected.\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langdetect'"
     ]
    }
   ],
   "source": [
    "# import langdetect\n",
    "\n",
    "# def detect_text_language(text):\n",
    "#   \"\"\"\n",
    "#   Detects the language of the given text using the langdetect library.\n",
    "\n",
    "#   Args:\n",
    "#     text: The text string to analyze.\n",
    "\n",
    "#   Returns:\n",
    "#     A string representing the detected language code (e.g., 'en', 'fr', 'es')\n",
    "#     or 'Unknown' if no language could be confidently detected.\n",
    "#   \"\"\"\n",
    "#   try:\n",
    "#     return langdetect.detect(text)\n",
    "#   except langdetect.LangDetectException:\n",
    "#     return 'Unknown'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "directory = Path.cwd().parents[1]\n",
    "data_dir = os.path.join(directory,\"data\")\n",
    "filepath = os.path.join(data_dir,\"x00.txt\")\n",
    "\n",
    "# with open(filepath,\"r\",encoding=\"utf-8\") as file:\n",
    "#     data = file.readlines()\n",
    "\n",
    "\n",
    "# def create_dataset(data):\n",
    "#     dataset = []\n",
    "    \n",
    "#     for record in tqdm(data):\n",
    "#         fields = record.split(\"\\t\")\n",
    "#         src_text = preprocess(fields[2])\n",
    "#         tgt_text = preprocess(fields[1])\n",
    "        \n",
    "#         dataset.append({\n",
    "#             \"src\": src_text,\n",
    "#             \"tgt\": tgt_text,\n",
    "#             \"src_lang\": detect_text_language(src_text),\n",
    "#             \"tgt_lang\": detect_text_language(tgt_text),\n",
    "#             \"src_len\": len(src_text),\n",
    "#             \"tgt_len\":  len(tgt_text)\n",
    "#         })\n",
    "    \n",
    "#     return dataset\n",
    "\n",
    "# dataset = create_dataset(data)\n",
    "\n",
    "# with open(os.path.join(data_dir,'translation_dataset_01.json'), 'w',encoding=\"utf-8\") as json_file:\n",
    "#     json.dump(dataset, json_file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.join(data_dir,\"translation_dataset_01.json\")\n",
    "\n",
    "raw_dataset = json.load(open(dataset_path,encoding=\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:00<00:00, 5041049.00it/s]\n"
     ]
    }
   ],
   "source": [
    "# Function to filter records by src_lang and tgt_lang\n",
    "def filter_records(data, src_lang, tgt_lang):\n",
    "    return [record for record in tqdm(data) if record['src_lang'] == src_lang and record['tgt_lang'] == tgt_lang]\n",
    "\n",
    "\n",
    "# Call the function to filter records where src_lang is 'en' and tgt_lang is 'bn'\n",
    "filtered_data = filter_records(raw_dataset, src_lang='en', tgt_lang='bn')\n",
    "\n",
    "# Print the filtered data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_sentences = [ data.get(\"tgt\") for data in filtered_data]\n",
    "src_sentences = [ data.get(\"src\") for data in filtered_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders, processors\n",
    "# tokenizer_path = os.path(data_dir,'bengali_bpe_tokenizer.json')\n",
    "\n",
    "def create_or_load_tokenizer(sentences: str , tokenizer_path: os.path, vocab_size:int =30000 , min_frequency: int =2):\n",
    "    # Check if the tokenizer file already exists\n",
    "    if os.path.exists(tokenizer_path):\n",
    "        # Load the existing tokenizer\n",
    "        tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "        print(f\"Tokenizer loaded from {tokenizer_path}\")\n",
    "    else:\n",
    "        # Initialize a BPE tokenizer\n",
    "        tokenizer = Tokenizer(models.BPE())\n",
    "\n",
    "        # Define a pre-tokenizer (to split on whitespace and punctuation)\n",
    "        tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "        # Define the BPE trainer\n",
    "        trainer = trainers.BpeTrainer(vocab_size=vocab_size, min_frequency=min_frequency, \n",
    "                                      special_tokens=[\"[PAD]\", \"[UNK]\", \"[SOS]\", \"[EOS]\"])\n",
    "\n",
    "        # Train the tokenizer on your Bengali dataset\n",
    "        tokenizer.train_from_iterator(sentences, trainer)\n",
    "\n",
    "        # Save the tokenizer for future use\n",
    "        tokenizer.save(tokenizer_path)\n",
    "        print(f\"Tokenizer trained and saved to {tokenizer_path}\")\n",
    "\n",
    "    return tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded from /Users/nityavg/Labs/ML4Interviews/data/en_src_bpe_tokenizer_01.json\n",
      "Tokenizer loaded from /Users/nityavg/Labs/ML4Interviews/data/bn_tgt_bpe_tokenizer_01.json\n"
     ]
    }
   ],
   "source": [
    "tokenizer_src = create_or_load_tokenizer(sentences=src_sentences, tokenizer_path= os.path.join(data_dir,\"en_src_bpe_tokenizer_01.json\"))\n",
    "tokenizer_tgt = create_or_load_tokenizer(sentences=tgt_sentences, tokenizer_path= os.path.join(data_dir,\"bn_tgt_bpe_tokenizer_01.json\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def casual_mask(size):\n",
    "    \"\"\"\n",
    "    Creates a casual mask for a sequence of length `size`.\n",
    "\n",
    "    Args:\n",
    "        size: The length of the sequence.\n",
    "\n",
    "    Returns:\n",
    "        A PyTorch tensor of shape (1, size, size) where:\n",
    "        - 1s represent valid connections (i.e., elements can see themselves and elements before them)\n",
    "        - 0s represent invalid connections (i.e., elements cannot see elements after them)\n",
    "\n",
    "    Example:\n",
    "        >>> casual_mask(5)\n",
    "        tensor([[[ True,  True,  True,  True,  True],\n",
    "                [False,  True,  True,  True,  True],\n",
    "                [False, False,  True,  True,  True],\n",
    "                [False, False, False,  True,  True],\n",
    "                [False, False, False, False,  True]]])\n",
    "    \"\"\"\n",
    "    mask  = torch.triu(torch.ones((1,size, size)),diagonal=1).type(torch.int)\n",
    "    return mask == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "# # It should contain both src and tgt sentences,\n",
    "# class MultilingualDataset(Dataset):\n",
    "#     def __init__(self, src_sentences,tgt_sentences, tokenizer_src, tokenizer_tgt, seq_len, dtype = torch.int64) -> None:\n",
    "#         super().__init__()\n",
    "#         self.seq_len = seq_len\n",
    "#         self.dtype = dtype\n",
    "\n",
    "#         self.src_sentences = src_sentences\n",
    "#         self.tgt_sentences = tgt_sentences\n",
    "#         self.tokenizer_src = tokenizer_src\n",
    "#         self.tokenizer_tgt = tokenizer_tgt\n",
    "\n",
    "#         self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=self.dtype)\n",
    "#         self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=self.dtype)\n",
    "#         self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=self.dtype)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.src_sentences)\n",
    "    \n",
    "#     def __getitem__(self, index):\n",
    "#         src_text = self.src_sentences[index]\n",
    "#         tgt_text = self.tgt_sentences[index]\n",
    "\n",
    "#         # Transform the text into tokens\n",
    "#         encoded_input_tokens = self.tokenizer_src.encode(src_text).ids\n",
    "#         decoded_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n",
    "       \n",
    "#         # Add [SOS],[EOS] and padding to each sentence\n",
    "#         enc_num_padding_tokens = self.seq_len - len(encoded_input_tokens) - 2 # for [SOS] and [EOS] token\n",
    "\n",
    "#         # For decoder add just [SOS]\n",
    "#         dec_num_padding_tokens = self.seq_len - len(decoded_input_tokens) - 1\n",
    "\n",
    "#         # Make sure the number of padding is not negative.\n",
    "#         if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n",
    "#             raise ValueError(f\"sentence is more than seq_len:{self.seq_len}\")\n",
    "        \n",
    "\n",
    "#         # Add [SOS],[EOS] token\n",
    "#         # Concatenate the [SOS], encoded tokens, [EOS] and padding tokens for the encoder input\n",
    "#         encoder_input = torch.cat(\n",
    "#             [\n",
    "#                 self.sos_token,  # Start of sentence token\n",
    "#                 torch.tensor(encoded_input_tokens,dtype=self.dtype),  # Encoded source sentence tokens\n",
    "#                 self.eos_token,  # End of sentence token\n",
    "#                 torch.tensor([self.pad_token]* enc_num_padding_tokens,dtype=self.dtype)  # Padding tokens to reach the desired sequence length\n",
    "#             ],\n",
    "#             dim= 0 \n",
    "#         )\n",
    "#         # Add only [S0S] token\n",
    "#         decoder_input = torch.cat(\n",
    "#             [\n",
    "#                 self.sos_token,\n",
    "#                 torch.tensor(decoded_input_tokens,dtype=self.dtype),\n",
    "#                 torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype= self.dtype)\n",
    "#             ],\n",
    "#             dim = 0\n",
    "#         )\n",
    "\n",
    "#         label = torch.cat(\n",
    "#             [\n",
    "#                 torch.tensor(decoded_input_tokens,dtype=self.dtype),\n",
    "#                 self.eos_token,\n",
    "#                 torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=self.dtype)\n",
    "\n",
    "#             ],\n",
    "#             dim = 0,\n",
    "#         )\n",
    "\n",
    "#         return {\n",
    "#             \"encoder_input\": encoder_input,\n",
    "#             \"decoder_input\" :  decoder_input,\n",
    "#             \"encoder_mask\" : (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(),\n",
    "#             \"decoder_mask\" : (decoder_input != self.pad_token).unsqueeze(0).int() & casual_mask(decoder_input.size(0)), # (1, seq_len) & (1, seq_len, seq_len),\n",
    "#             \"label\": label,  # (seq_len)\n",
    "#             \"src_text\": src_text,\n",
    "#             \"tgt_text\" : tgt_text\n",
    "            \n",
    "#         }\n",
    "\n",
    "def causal_mask(seq_len: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Creates a causal mask for self-attention.\n",
    "    Shape: (1, seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    mask = torch.tril(torch.ones((seq_len, seq_len), dtype=torch.int))\n",
    "    return mask.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "\n",
    "class MultilingualDataset(Dataset):\n",
    "    def __init__(self, src_sentences, tgt_sentences, tokenizer_src, tokenizer_tgt, seq_len, dtype=torch.int64) -> None:\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.dtype = dtype\n",
    "\n",
    "        self.src_sentences = src_sentences\n",
    "        self.tgt_sentences = tgt_sentences\n",
    "        self.tokenizer_src = tokenizer_src\n",
    "        self.tokenizer_tgt = tokenizer_tgt\n",
    "\n",
    "        self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=self.dtype)\n",
    "        self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=self.dtype)\n",
    "        self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=self.dtype)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_sentences)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        src_text = self.src_sentences[index]\n",
    "        tgt_text = self.tgt_sentences[index]\n",
    "\n",
    "        # Tokenize\n",
    "        encoded_input_tokens = self.tokenizer_src.encode(src_text).ids\n",
    "        decoded_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n",
    "       \n",
    "        # Padding calculation\n",
    "        enc_num_padding_tokens = self.seq_len - len(encoded_input_tokens) - 2  # [SOS], [EOS]\n",
    "        dec_num_padding_tokens = self.seq_len - len(decoded_input_tokens) - 1  # [SOS]\n",
    "\n",
    "        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n",
    "            raise ValueError(f\"Sentence is longer than seq_len: {self.seq_len}\")\n",
    "        \n",
    "        # Encoder input: [SOS] ... [EOS] [PAD...]\n",
    "        encoder_input = torch.cat(\n",
    "            [self.sos_token,\n",
    "             torch.tensor(encoded_input_tokens, dtype=self.dtype),\n",
    "             self.eos_token,\n",
    "             self.pad_token.repeat(enc_num_padding_tokens)],\n",
    "            dim=0\n",
    "        )\n",
    "\n",
    "        # Decoder input: [SOS] ... [PAD...]\n",
    "        decoder_input = torch.cat(\n",
    "            [self.sos_token,\n",
    "             torch.tensor(decoded_input_tokens, dtype=self.dtype),\n",
    "             self.pad_token.repeat(dec_num_padding_tokens)],\n",
    "            dim=0\n",
    "        )\n",
    "\n",
    "        # Labels: ... [EOS] [PAD...]\n",
    "        label = torch.cat(\n",
    "            [torch.tensor(decoded_input_tokens, dtype=self.dtype),\n",
    "             self.eos_token,\n",
    "             self.pad_token.repeat(dec_num_padding_tokens)],\n",
    "            dim=0\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"encoder_input\": encoder_input,\n",
    "            \"decoder_input\": decoder_input,\n",
    "            \"encoder_mask\": (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(),\n",
    "            \"decoder_mask\": (decoder_input != self.pad_token).unsqueeze(0).unsqueeze(1).int() & causal_mask(decoder_input.size(0)),\n",
    "            \"label\": label,\n",
    "            \"src_text\": src_text,\n",
    "            \"tgt_text\": tgt_text\n",
    "        }    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = MultilingualDataset(src_sentences=src_sentences,tgt_sentences=tgt_sentences,tokenizer_src=tokenizer_src,tokenizer_tgt=tokenizer_tgt,seq_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'encoder_input': tensor([    2,  1387,  2387,  2273,  3192,  1417,  1515,  1616,  1377,  1345,\n",
       "         26048,  1355,  2273,  6177,    17,     3,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]),\n",
       " 'decoder_input': tensor([    2,  5719, 13178,  1431,  8971,  7467,  1335,  4872, 15856,  1990,\n",
       "         12349,  1309,  1286,   530,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]),\n",
       " 'encoder_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0]]], dtype=torch.int32),\n",
       " 'decoder_mask': tensor([[[1, 0, 0,  ..., 0, 0, 0],\n",
       "          [1, 1, 0,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0]]], dtype=torch.int32),\n",
       " 'label': tensor([ 5719, 13178,  1431,  8971,  7467,  1335,  4872, 15856,  1990, 12349,\n",
       "          1309,  1286,   530,     3,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]),\n",
       " 'src_text': 'The six official languages are also used for the dissemination of official documents.',\n",
       " 'tgt_text': 'অফিসিয়াল ডকুমেন্টগুলো প্রচারের জন্যও এই ছয়টি অফিশিয়াল ভাষা ব্যাবহার করা হয়।'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[122]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "def test_train_split(dataset: Dataset,prob : float = 0.9):\n",
    "    \"\"\"Splits a dataset into training and validation sets.\n",
    "\n",
    "    Args:\n",
    "        dataset (Dataset): The dataset to split.\n",
    "        prob (float, optional): The proportion of the dataset to use for training. Defaults to 0.9.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the training and validation datasets.\n",
    "    \"\"\"\n",
    "    # Keep 90% for training, 10% for validation\n",
    "    train_ds_size = int(prob * len(dataset))\n",
    "    val_ds_size = len(dataset) - train_ds_size\n",
    "    train_ds, val_ds = random_split(dataset, [train_ds_size, val_ds_size])\n",
    "    return train_ds, val_ds\n",
    "\n",
    "def get_src_tgt_sentences(ds:Dataset):\n",
    "    \"\"\"Extracts source and target sentences from a dataset.\n",
    "\n",
    "    Args:\n",
    "        ds (Dataset): The dataset to extract sentences from.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the source and target sentences.\n",
    "    \"\"\"\n",
    "    src_sentences = [ sent[0] for sent in ds.dataset]\n",
    "    tgt_sentences = [ sent[1] for sent in ds.dataset]\n",
    "    return src_sentences, tgt_sentences\n",
    "\n",
    "def get_dataset(dataset:Dataset,batch_size = 7):\n",
    "    \"\"\"Prepares the dataset for training and validation.\n",
    "\n",
    "    Args:\n",
    "        dataset (Dataset): The dataset to prepare.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the training and validation datasets.\n",
    "    \"\"\"\n",
    "    filtered_data = filter_records(dataset, src_lang='en', tgt_lang='bn')  # Filter records based on languages\n",
    "    tgt_sentences = [ data.get(\"tgt\") for data in filtered_data]\n",
    "    src_sentences = [ data.get(\"src\") for data in filtered_data]\n",
    "\n",
    "    tokenizer_src = create_or_load_tokenizer(sentences=src_sentences, tokenizer_path= os.path.join(data_dir,\"en_src_bpe_tokenizer_01.json\"))\n",
    "    tokenizer_tgt = create_or_load_tokenizer(sentences=tgt_sentences, tokenizer_path= os.path.join(data_dir,\"bn_tgt_bpe_tokenizer_01.json\"))\n",
    "\n",
    "    train_ds, val_ds = test_train_split(list(zip(src_sentences,tgt_sentences)))  # Split into train and validation sets\n",
    "    train_src_ds, train_tgt_ds = get_src_tgt_sentences(train_ds)  # Extract source and target sentences from train set\n",
    "    val_src_ds, val_tgt_ds = get_src_tgt_sentences(val_ds)  # Extract source and target sentences from validation set\n",
    "    train_ds = MultilingualDataset(src_sentences=train_src_ds,tgt_sentences=train_tgt_ds,tokenizer_src=tokenizer_src,tokenizer_tgt=tokenizer_tgt,seq_len=512)\n",
    "    val_ds = MultilingualDataset(src_sentences=val_src_ds,tgt_sentences=val_tgt_ds,tokenizer_src=tokenizer_src,tokenizer_tgt=tokenizer_tgt,seq_len=512)\n",
    "    \n",
    "    # Find the max len of each sentence in source and target sentences\n",
    "    max_len_src = 0\n",
    "    max_len_tgt = 0\n",
    "\n",
    "    for item in list(zip(src_sentences,tgt_sentences)):\n",
    "        src_ids = tokenizer_src.encode(item[0]).ids\n",
    "        tgt_ids = tokenizer_tgt.encode(item[1]).ids\n",
    "        max_len_src = max(max_len_src,len(src_ids))\n",
    "        max_len_tgt = max(max_len_tgt,len(tgt_ids))\n",
    "    \n",
    "    print(f'Max length of source sentence: {max_len_src}')\n",
    "    print(f'Max length of target sentence: {max_len_tgt}')\n",
    "\n",
    "    train_dataloader = DataLoader(train_ds,batch_size=batch_size,shuffle=True)\n",
    "    val_dataloader = DataLoader(val_ds,batch_size=1, shuffle=True)\n",
    "\n",
    "    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:00<00:00, 4103533.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded from /Users/nityavg/Labs/ML4Interviews/data/en_src_bpe_tokenizer_01.json\n",
      "Tokenizer loaded from /Users/nityavg/Labs/ML4Interviews/data/bn_tgt_bpe_tokenizer_01.json\n",
      "Max length of source sentence: 2603\n",
      "Max length of target sentence: 1813\n"
     ]
    }
   ],
   "source": [
    "train_dl, val_dl, tokenizer_src, tokenizer_tgt = get_dataset(raw_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'encoder_input': tensor([    2, 14182,  1614,  1417,  8131,  1424,  1546,  4204,    17,     3,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]),\n",
       " 'decoder_input': tensor([    2,  3624,  1530,  1409, 12266,  8428,  1264,  4641,  1273,   530,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]),\n",
       " 'encoder_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0]]], dtype=torch.int32),\n",
       " 'decoder_mask': tensor([[[1, 0, 0,  ..., 0, 0, 0],\n",
       "          [1, 1, 0,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0]]], dtype=torch.int32),\n",
       " 'label': tensor([ 3624,  1530,  1409, 12266,  8428,  1264,  4641,  1273,   530,     3,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]),\n",
       " 'src_text': 'Rather they are alive with their Lord.',\n",
       " 'tgt_text': 'বরং তারা তাদের পালনকর্তার সাক্ষাতকে অস্বীকার করে।'}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dl.dataset.__getitem__(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(vocab_src_len, vocab_tgt_len):\n",
    "    model = build_transformer(src_vocab_size=vocab_src_len,\n",
    "                              tgt_vocab_size=vocab_tgt_len,\n",
    "                              src_seq_len= 512,\n",
    "                              tgt_seq_len=512,\n",
    "                              model_dim= 512)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def get_config():\n",
    "    return {\n",
    "        \"batch_size\": 4,\n",
    "        \"num_epochs\": 20,\n",
    "        \"lr\": 10**-4,\n",
    "        \"seq_len\": 350,\n",
    "        \"model_dim\": 512,\n",
    "        \"datasource\" : \"en_bn_open\",\n",
    "        \"model_folder\": \"weights\",\n",
    "        \"model_basename\": \"tmodel_\",\n",
    "        \"preload\": \"latest\",\n",
    "        \"tokenizer_file\": \"tokenizer_{0}.json\",\n",
    "        \"experiment_name\": \"runs/tmodel\"\n",
    "    }\n",
    "\n",
    "def get_weights_file_path(config, epoch: str):\n",
    "    model_folder = f\"{config['datasource']}_{config['model_folder']}\"\n",
    "    model_filename = f\"{config['model_basename']}{epoch}.pt\"\n",
    "    return str(Path('.') / model_folder / model_filename)\n",
    "\n",
    "# Find the latest weights file in the weights folder\n",
    "def latest_weights_file_path(config):\n",
    "    model_folder = f\"{config['datasource']}_{config['model_folder']}\"\n",
    "    model_filename = f\"{config['model_basename']}*\"\n",
    "    weights_files = list(Path(model_folder).glob(model_filename))\n",
    "    if len(weights_files) == 0:\n",
    "        return None\n",
    "    weights_files.sort()\n",
    "    return str(weights_files[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_validation(model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, print_msg, global_step, writer, num_examples=2):\n",
    "    model.eval()\n",
    "    count = 0\n",
    "\n",
    "    source_texts = []\n",
    "    expected = []\n",
    "    predicted = []\n",
    "\n",
    "    try:\n",
    "        # get the console window width\n",
    "        with os.popen('stty size', 'r') as console:\n",
    "            _, console_width = console.read().split()\n",
    "            console_width = int(console_width)\n",
    "    except:\n",
    "        # If we can't get the console width, use 80 as default\n",
    "        console_width = 80\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in validation_ds:\n",
    "            count += 1\n",
    "            encoder_input = batch[\"encoder_input\"].to(device) # (b, seq_len)\n",
    "            encoder_mask = batch[\"encoder_mask\"].to(device) # (b, 1, 1, seq_len)\n",
    "\n",
    "            # check that the batch size is 1\n",
    "            assert encoder_input.size(\n",
    "                0) == 1, \"Batch size must be 1 for validation\"\n",
    "\n",
    "            model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
    "\n",
    "            source_text = batch[\"src_text\"][0]\n",
    "            target_text = batch[\"tgt_text\"][0]\n",
    "            model_out_text = tokenizer_tgt.decode(model_out.detach().cpu().numpy())\n",
    "\n",
    "            source_texts.append(source_text)\n",
    "            expected.append(target_text)\n",
    "            predicted.append(model_out_text)\n",
    "            \n",
    "            # Print the source, target and model output\n",
    "            print_msg('-'*console_width)\n",
    "            print_msg(f\"{f'SOURCE: ':>12}{source_text}\")\n",
    "            print_msg(f\"{f'TARGET: ':>12}{target_text}\")\n",
    "            print_msg(f\"{f'PREDICTED: ':>12}{model_out_text}\")\n",
    "\n",
    "            if count == num_examples:\n",
    "                print_msg('-'*console_width)\n",
    "                break\n",
    "    \n",
    "    if writer:\n",
    "        # Evaluate the character error rate\n",
    "        # Compute the char error rate \n",
    "        metric = torchmetrics.CharErrorRate()\n",
    "        cer = metric(predicted, expected)\n",
    "        writer.add_scalar('validation cer', cer, global_step)\n",
    "        writer.flush()\n",
    "\n",
    "        # Compute the word error rate\n",
    "        metric = torchmetrics.WordErrorRate()\n",
    "        wer = metric(predicted, expected)\n",
    "        writer.add_scalar('validation wer', wer, global_step)\n",
    "        writer.flush()\n",
    "\n",
    "        # Compute the BLEU metric\n",
    "        metric = torchmetrics.BLEUScore()\n",
    "        bleu = metric(predicted, expected)\n",
    "        writer.add_scalar('validation BLEU', bleu, global_step)\n",
    "        writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def train_model(config,dataset):\n",
    "    # Define the device\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.has_mps or torch.backends.mps.is_available() else \"cpu\"\n",
    "    print(\"Using device:\", device)\n",
    "    if (device == 'cuda'):\n",
    "        print(f\"Device name: {torch.cuda.get_device_name(device.index)}\")\n",
    "        print(f\"Device memory: {torch.cuda.get_device_properties(device.index).total_memory / 1024 ** 3} GB\")\n",
    "    elif (device == 'mps'):\n",
    "        print(f\"Device name: <mps>\")\n",
    "    else:\n",
    "        print(\"NOTE: If you have a GPU, consider using it for training.\")\n",
    "        print(\"      On a Windows machine with NVidia GPU, check this video: https://www.youtube.com/watch?v=GMSjDTU8Zlc\")\n",
    "        print(\"      On a Mac machine, run: pip3 install --pre torch torchvision torchaudio torchtext --index-url https://download.pytorch.org/whl/nightly/cpu\")\n",
    "    device = torch.device(device)\n",
    "\n",
    "    # Make sure the weights folder exists\n",
    "    Path(f\"{config['datasource']}_{config['model_folder']}\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_dataset(dataset)\n",
    "    model = get_model(tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
    "    # Tensorboard\n",
    "    writer = SummaryWriter(config['experiment_name'])\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps=1e-9)\n",
    "\n",
    "    # If the user specified a model to preload before training, load it\n",
    "    initial_epoch = 0\n",
    "    global_step = 0\n",
    "    preload = config['preload']\n",
    "    model_filename = latest_weights_file_path(config) if preload == 'latest' else get_weights_file_path(config, preload) if preload else None\n",
    "    if model_filename:\n",
    "        print(f'Preloading model {model_filename}')\n",
    "        state = torch.load(model_filename)\n",
    "        model.load_state_dict(state['model_state_dict'])\n",
    "        initial_epoch = state['epoch'] + 1\n",
    "        optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "        global_step = state['global_step']\n",
    "    else:\n",
    "        print('No model to preload, starting from scratch')\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_src.token_to_id('[PAD]'), label_smoothing=0.1).to(device)\n",
    "\n",
    "    for epoch in range(initial_epoch, config['num_epochs']):\n",
    "        torch.cuda.empty_cache()\n",
    "        model.train()\n",
    "        batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n",
    "        for batch in batch_iterator:\n",
    "\n",
    "            encoder_input = batch['encoder_input'].to(device) # (b, seq_len)\n",
    "            decoder_input = batch['decoder_input'].to(device) # (B, seq_len)\n",
    "            encoder_mask = batch['encoder_mask'].to(device) # (B, 1, 1, seq_len)\n",
    "            decoder_mask = batch['decoder_mask'].to(device) # (B, 1, seq_len, seq_len)\n",
    "\n",
    "            # Run the tensors through the encoder, decoder and the projection layer\n",
    "            encoder_output = model.encode(encoder_input, encoder_mask) # (B, seq_len, d_model)\n",
    "            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask) # (B, seq_len, d_model)\n",
    "            proj_output = model.project(decoder_output) # (B, seq_len, vocab_size)\n",
    "\n",
    "            # Compare the output with the label\n",
    "            label = batch['label'].to(device) # (B, seq_len)\n",
    "\n",
    "            # Compute the loss using a simple cross entropy\n",
    "            loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n",
    "            batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n",
    "\n",
    "            # Log the loss\n",
    "            writer.add_scalar('train loss', loss.item(), global_step)\n",
    "            writer.flush()\n",
    "\n",
    "            # Backpropagate the loss\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the weights\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "        # Run validation at the end of every epoch\n",
    "        run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, lambda msg: batch_iterator.write(msg), global_step, writer)\n",
    "\n",
    "        # Save the model at the end of every epoch\n",
    "        model_filename = get_weights_file_path(config, f\"{epoch:02d}\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'global_step': global_step\n",
    "        }, model_filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5h/3c53wmw52x1cpkkxvsg9xwtr0000gn/T/ipykernel_22556/1570122189.py:6: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.has_mps or torch.backends.mps.is_available() else \"cpu\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Device name: <mps>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:00<00:00, 3873393.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded from /Users/nityavg/Labs/ML4Interviews/data/en_src_bpe_tokenizer_01.json\n",
      "Tokenizer loaded from /Users/nityavg/Labs/ML4Interviews/data/bn_tgt_bpe_tokenizer_01.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of source sentence: 2603\n",
      "Max length of target sentence: 1813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5h/3c53wmw52x1cpkkxvsg9xwtr0000gn/T/ipykernel_22556/314797125.py:42: FutureWarning: `nn.init.xavier_uniform` is now deprecated in favor of `nn.init.xavier_uniform_`.\n",
      "  nn.init.xavier_uniform(p) # Initialize parameters using Xavier uniform initialization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No model to preload, starting from scratch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 00:  32%|███▏      | 4121/12990 [1:13:36<2:38:24,  1.07s/it, loss=7.669]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Sentence is longer than seq_len: 512",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[72]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m config = get_config()\n\u001b[32m      6\u001b[39m dataset = raw_dataset\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[71]\u001b[39m\u001b[32m, line 50\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(config, dataset)\u001b[39m\n\u001b[32m     48\u001b[39m model.train()\n\u001b[32m     49\u001b[39m batch_iterator = tqdm(train_dataloader, desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProcessing Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_iterator\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_input\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mencoder_input\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# (b, seq_len)\u001b[39;49;00m\n\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_input\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdecoder_input\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# (B, seq_len)\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Labs/ML4Interviews/venv/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Labs/ML4Interviews/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:734\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    732\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    733\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m734\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    736\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    737\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    739\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    740\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Labs/ML4Interviews/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:790\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    788\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    789\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m790\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    791\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    792\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Labs/ML4Interviews/venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[62]\u001b[39m\u001b[32m, line 125\u001b[39m, in \u001b[36mMultilingualDataset.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    122\u001b[39m dec_num_padding_tokens = \u001b[38;5;28mself\u001b[39m.seq_len - \u001b[38;5;28mlen\u001b[39m(decoded_input_tokens) - \u001b[32m1\u001b[39m  \u001b[38;5;66;03m# [SOS]\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m enc_num_padding_tokens < \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m dec_num_padding_tokens < \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSentence is longer than seq_len: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.seq_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    127\u001b[39m \u001b[38;5;66;03m# Encoder input: [SOS] ... [EOS] [PAD...]\u001b[39;00m\n\u001b[32m    128\u001b[39m encoder_input = torch.cat(\n\u001b[32m    129\u001b[39m     [\u001b[38;5;28mself\u001b[39m.sos_token,\n\u001b[32m    130\u001b[39m      torch.tensor(encoded_input_tokens, dtype=\u001b[38;5;28mself\u001b[39m.dtype),\n\u001b[32m   (...)\u001b[39m\u001b[32m    133\u001b[39m     dim=\u001b[32m0\u001b[39m\n\u001b[32m    134\u001b[39m )\n",
      "\u001b[31mValueError\u001b[39m: Sentence is longer than seq_len: 512"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "                                   \n",
    "if __name__ == '__main__':\n",
    "    config = get_config()\n",
    "    dataset = raw_dataset\n",
    "    train_model(config=config,dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
