{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers - Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as torch_data\n",
    "import math\n",
    "import copy\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Embeddings\n",
    "\n",
    "\n",
    "In essence, InputEmbeddings is the first step in a Transformer model that converts input data (like words in a sentence) into a format that the neural network can understand and process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Embedding: The core of this class is the nn.Embedding layer from PyTorch. This layer is responsible for representing each unique word (or token) in your vocabulary as a dense vector. These vectors capture semantic information about the words, making them suitable for processing by the Transformer.\n",
    "\n",
    "* Model Dimension: The model_dim parameter specifies the size of the embedding vectors. This dimension dictates how much information about each word is encoded in the embedding. A larger model dimension allows for more complex representations but also increases computational requirements.\n",
    "\n",
    "* Vocabulary Size: The vocab_size parameter determines how many unique words (or tokens) your model needs to represent. Each word in your vocabulary gets its own unique embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn as nn\n",
    "\n",
    "class InputEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    This class defines the input embedding layer for a neural network. \n",
    "    It converts input words (represented as indices) into dense vector representations (embeddings).\n",
    "    \"\"\"\n",
    "    def __init__(self, model_dim: int, vocab_size: int) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the InputEmbeddings module.\n",
    "\n",
    "        Args:\n",
    "            model_dim (int): The dimensionality of the embedding vectors.\n",
    "            vocab_size (int): The size of the vocabulary.\n",
    "        \"\"\"\n",
    "        super().__init__() # Calls Parent's constructor , i.e. nn.Module\n",
    "        self.model_dim = model_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size,embedding_dim=model_dim)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        Performs the forward pass of the module.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor representing a sequence of word indices.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Embedded representation of the input sequence.\n",
    "        \"\"\"\n",
    "        # This is the core function of the class, defining how the module processes input data.\n",
    "        # It takes an input tensor x, representing a sequence of words (usually indices into the vocabulary).\n",
    "        # It uses the embedding layer to lookup the corresponding embedding vector for each word in x.\n",
    "        # Scaling: The result is then multiplied by the square root of model_dim. \n",
    "        # NOTE : It helps with numerical stability and prevents the gradients from vanishing or exploding during training.\n",
    "        \n",
    "        return self.embedding(x) * math.sqrt(self.model_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The cat sat on the mat.\"\n",
    "\n",
    "\"\"\"\n",
    "First, you'd convert this sentence into a sequence of integers, \n",
    "representing each word's index in your vocabulary. \n",
    "Let's say:\n",
    "\"\"\"\n",
    "\n",
    "sentence_indices = [1, 2, 3, 4, 1, 5]  \n",
    "\n",
    "embeddings = InputEmbeddings(model_dim=512, vocab_size=10000)  # Example values\n",
    "# PyTorch automatically invokes the forward() method behind the scenes.\n",
    "embedded_sentence = embeddings(torch.tensor(sentence_indices)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 512])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_sentence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-59.7526,   2.0702,  30.2526,  ...,  16.4866,   7.7719,   9.4011],\n",
       "        [-10.5886,  20.0774, -23.9372,  ...,  -4.9390,  21.6019,  24.8528],\n",
       "        [ 16.3475, -16.2876,  15.4052,  ..., -34.1315,  26.8383, -33.6186],\n",
       "        [ -1.9766, -28.5220, -15.9653,  ...,  11.7164,  30.1072,  11.5750],\n",
       "        [-59.7526,   2.0702,  30.2526,  ...,  16.4866,   7.7719,   9.4011],\n",
       "        [ 52.3079, -11.6581, -46.3877,  ..., -16.4009,  23.9733,  45.4135]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summary\n",
    "The InputEmbeddings class effectively converts your input data from raw words or indices into dense vectors that the Transformer model can understand and manipulate. \n",
    "These vectors are crucial for the Transformer to learn relationships between words, understand the context of sentences, and ultimately perform tasks like translation, text generation, or question answering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <b>Importance</b>\n",
    "Positional encoding is essential for Transformers to understand the order of words in a sequence. Without it, the model wouldn't be able to differentiate between \"The cat sat on the mat\" and \"Mat the on sat cat the.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where:\n",
    "\n",
    "- pos is the position of the word in the sequence.\n",
    "- i is the dimension index.\n",
    "- d_model is the size of the word embeddings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, model_dim:int, seq_len: int,dropout:float) -> None:\n",
    "        super().__init__()\n",
    "        self.model_dim = model_dim\n",
    "        self.seq_length = seq_len\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        #create a matrix of shape (seq_len, model_dim)\n",
    "        positional_encoder = torch.zeros(seq_len,model_dim)\n",
    "\n",
    "        # create a vector of shape(seq_len)\n",
    "        position_vector = torch.arange(0,seq_len,dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "        # create a vector of shape(d_model)\n",
    "        div_term = torch.exp(torch.arange(0,model_dim,2).float() * (-math.log(10000.0)/model_dim))\n",
    "\n",
    "        # Apply sine to even indexes\n",
    "        positional_encoder[:,0::2] = torch.sin(position_vector * div_term)\n",
    "\n",
    "        # Apply cosine to odd indexes\n",
    "        positional_encoder[:,1::2] = torch.cos(position_vector * div_term)\n",
    "\n",
    "        ## Add a batch dimension to positional encoding\n",
    "        positional_encoder = positional_encoder.unsqueeze(0)\n",
    "\n",
    "        # Register the encoding as buffer\n",
    "        self.register_buffer('pe',positional_encoder)\n",
    "\n",
    "    \"\"\"\n",
    "    The forward method takes the output of the input embeddings (x) as input.\n",
    "    It adds the positional encoding matrix (sliced to the length of the input sequence) \n",
    "    to the input embeddings.\n",
    "    The resulting tensor is passed through the dropout layer to prevent overfitting.\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = x +  (self.pe[:,:x.shape[1],:]).requires_grad_(False)\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Indices: tensor([[195,  43,  54, 415, 262, 312, 812, 253,  53, 587, 383,  18, 813, 202,\n",
      "         720, 659, 667, 721, 606, 627]])\n",
      "Embedded Input Shape: torch.Size([1, 20, 256])\n",
      "Encoded Input Shape: torch.Size([1, 20, 256])\n",
      "Encoded Input (First Few Elements):\n",
      " tensor([[  2.7631,   4.7577,   3.2323,  ...,  -7.5638, -16.9468,  10.6443],\n",
      "        [ -4.7211,  -2.2727, -12.9234,  ...,  23.8853,  -6.9061, -14.6514],\n",
      "        [ 26.0332,   2.5981,  -3.4918,  ...,  14.9398,  28.6802,   3.3275],\n",
      "        [-23.2218,  15.2911, -10.9084,  ...,  -3.6486,  24.3930, -17.6850],\n",
      "        [ -7.1676, -11.0623,   7.7219,  ..., -38.3695,  26.7025,   0.0000]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Set parameters\n",
    "vocab_size = 1000  # Example vocabulary size\n",
    "model_dim = 256    # Embedding dimension\n",
    "seq_length = 20  # Maximum sequence length\n",
    "dropout = 0.1    # Dropout probability\n",
    "\n",
    "# Create instances of the modules\n",
    "input_embeddings = InputEmbeddings(model_dim, vocab_size)\n",
    "positional_encoding = PositionalEncoding(model_dim, seq_length, dropout)\n",
    "\n",
    "# Sample input (random indices into the vocabulary)\n",
    "input_indices = torch.randint(0, vocab_size, (1, seq_length))\n",
    "\n",
    "# Perform the embedding and positional encoding\n",
    "embedded_input = input_embeddings(input_indices)\n",
    "encoded_input = positional_encoding(embedded_input)\n",
    "\n",
    "# Print the results\n",
    "print(\"Input Indices:\", input_indices)\n",
    "print(\"Embedded Input Shape:\", embedded_input.shape)\n",
    "print(\"Encoded Input Shape:\", encoded_input.shape)\n",
    "\n",
    "# You can also visualize the encoded input to see how positional information is added\n",
    "print(\"Encoded Input (First Few Elements):\\n\", encoded_input[0, :5, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiHead-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1,..., \\text{head}_h) W^O\n",
    "$$\n",
    "$$\n",
    "\\text{where}, \\quad \\text{head}_i = \\text{Attention}(Q W_Q^i, K W_K^i, V W_V^i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaled Dot-Product Attention\n",
    "\n",
    "\\begin{align}\n",
    "\\text Attention(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, model_dim:int, h:int, dropout:float) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding Model Size\n",
    "        self.model_dim = model_dim\n",
    "        # Number of heads\n",
    "        self.h = h\n",
    "\n",
    "        # Make sure d_model is divisible by h\n",
    "        assert model_dim % h == 0, \"d_model is not divisible by h\"\n",
    "\n",
    "        # Initializing dimensions\n",
    "        ## Dimension of vector seen by each head\n",
    "        self.d_k = self.model_dim // h\n",
    "        # Wq, Wk, Wv, Wo - Linear layers for query, key, value and output transformations\n",
    "        self.w_q = nn.Linear(model_dim,model_dim,bias=False)\n",
    "        self.w_k = nn.Linear(model_dim,model_dim,bias=False)\n",
    "        self.w_v = nn.Linear(model_dim,model_dim,bias=False)\n",
    "        self.w_o = nn.Linear(model_dim,model_dim,bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(query,key,value,mask, dropout : nn.Dropout):\n",
    "        \"\"\"\n",
    "        Scaled Dot-Product Attention\n",
    "\n",
    "        Args:\n",
    "            query (torch.Tensor): Query tensor of shape (batch_size, seq_len, model_dim).\n",
    "            key (torch.Tensor): Key tensor of shape (batch_size, seq_len, model_dim).\n",
    "            value (torch.Tensor): Value tensor of shape (batch_size, seq_len, model_dim).\n",
    "            mask (torch.Tensor): Mask tensor of shape (batch_size, seq_len), where 1 indicates a valid position and 0 indicates a masked position.\n",
    "            dropout (nn.Dropout): _description_\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        # Calculate dimension of the query vector\n",
    "        d_k = query.shape[-1]\n",
    "        # Perform scaled dot-product attention here... \n",
    "        # (Calculations for attention weights and output)\n",
    "        # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len) \n",
    "        attention_scores = (query @ key.transpose(-2,-1))/ math.sqrt(d_k)\n",
    "\n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            # ref: https://pytorch.org/docs/stable/generated/torch.Tensor.masked_fill_.html\n",
    "            # Set the attention scores corresponding to masked positions to a very low value (-1e9).\n",
    "            # This ensures that the model does not attend to the masked positions.\n",
    "            attention_scores.masked_fill_(mask==0,-1e9)\n",
    "        \n",
    "        # Normalize attention scores using softmax\n",
    "        attention_scores = attention_scores.softmax(dim=-1)\n",
    "\n",
    "        # Apply dropout to attention scores if dropout is not None\n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "        \n",
    "        # Multiply attention scores by the value matrix to get the output\n",
    "        return (attention_scores @ value) , attention_scores\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "        \"\"\"\n",
    "        Forward pass of the Multi-Head Attention layer.\n",
    "\n",
    "        Args:\n",
    "            q (torch.Tensor): Query tensor of shape (batch_size, seq_len, model_dim).\n",
    "            k (torch.Tensor): Key tensor of shape (batch_size, seq_len, model_dim).\n",
    "            v (torch.Tensor): Value tensor of shape (batch_size, seq_len, model_dim).\n",
    "            mask (torch.Tensor): Mask tensor of shape (batch_size, seq_len), where 1 indicates a valid position and 0 indicates a masked position.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, seq_len, model_dim).\n",
    "        \"\"\"\n",
    "\n",
    "        # Apply linear transformations to query, key, and value tensors\n",
    "        query = self.w_q(q)\n",
    "        key = self.w_k(k)\n",
    "        value = self.w_v(v)\n",
    "\n",
    "        # Reshape and transpose query, key, and value tensors to prepare for multi-head attention\n",
    "        query = query.view(query.shape[0],query.shape[1],self.h,self.d_k).transpose(1,2)\n",
    "        key = key.view(key.shape[0],key.shape[1],self.h,self.d_k).transpose(1,2)\n",
    "        value = value.view(value.shape[0],value.shape[1],self.h,self.d_k).transpose(1,2)\n",
    "        \n",
    "        # Calculate attention\n",
    "        # The static method 'attention' is used here to perform the scaled dot-product attention\n",
    "        # The returned value 'x' is the output of the attention mechanism\n",
    "        # 'self.attention_scores' stores the attention weights for later use\n",
    "        x, self.attention_scores = MultiHeadAttention.attention(query,key, value, mask, self.dropout)\n",
    "\n",
    "        # Combine all the heads together\n",
    "        # The output 'x' is reshaped and transposed to combine the outputs of all heads\n",
    "        # The output 'x' now has the shape (batch_size, seq_len, model_dim)\n",
    "        x = x.transpose(1,2).contiguous().view(x.shape[0],-1, self.h * self.d_k)\n",
    "\n",
    "        # Multiply by Wo\n",
    "        # The final output is obtained by applying a linear transformation to the combined output 'x'\n",
    "        return self.w_o(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Output Shape: torch.Size([1, 20, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ -2.9639,  -7.5714,  -2.0499,  ...,  10.8611,  -7.5002,   4.3730],\n",
       "         [  0.0215,  -9.4806,  11.4350,  ...,  -9.9990,   3.4916,   5.2698],\n",
       "         [  0.4006,   8.3645,  -5.5867,  ...,   0.2895,   0.8181,  -2.7753],\n",
       "         ...,\n",
       "         [  1.1358,   0.5760,   5.2000,  ...,  -5.3769,  -1.4006,  -3.6483],\n",
       "         [-11.6491,  14.9184,   8.0676,  ...,  -3.1035,   4.4810,   6.1029],\n",
       "         [  8.1648,   9.2645,   6.0542,  ...,   2.9249,  -2.6227,   6.6580]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set parameters\n",
    "vocab_size = 1000\n",
    "model_dim = 256\n",
    "seq_length = 20\n",
    "h = 8      # Number of attention heads\n",
    "dropout = 0.1\n",
    "\n",
    "# Create instances of the modules\n",
    "input_embeddings = InputEmbeddings(model_dim, vocab_size)\n",
    "positional_encoding = PositionalEncoding(model_dim, seq_length, dropout)\n",
    "multihead_attention = MultiHeadAttention(model_dim, h, dropout)\n",
    "\n",
    "# Sample input (random indices)\n",
    "input_indices = torch.randint(0, vocab_size, (1, seq_length))\n",
    "\n",
    "# Embed and encode the input\n",
    "embedded_input = input_embeddings(input_indices)\n",
    "encoded_input = positional_encoding(embedded_input)\n",
    "\n",
    "# Perform multi-head attention (using encoded input as query, key, and value)\n",
    "attention_output = multihead_attention(encoded_input, encoded_input, encoded_input, None)  # No mask in this example\n",
    "\n",
    "print(\"Attention Output Shape:\", attention_output.shape)  # (batch_size, seq_len, model_dim)\n",
    "attention_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LayerNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int , eps: float= 10 ** -6) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the LayerNormalization module.\n",
    "\n",
    "        Args:\n",
    "            features (int): The number of features in the input tensor.\n",
    "            eps (float, optional): A small value added to the denominator to prevent division by zero. Defaults to 10^-6.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        # alpha and bias are learnable parameters\n",
    "        self.alpha = nn.Parameter(torch.ones(features))\n",
    "        self.bias = nn.Parameter(torch.ones(features))\n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        Applies LayerNormalization to the input tensor.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor with shape (batch_size, seq_len, hidden_size).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The normalized output tensor with the same shape as the input.\n",
    "        \"\"\"\n",
    "        #x :( batch_size, seq_len, hidden_size)\n",
    "        \n",
    "        mean = x.mean(dim = -1, keepdim = True) #(batch,seq_len, 1)\n",
    "        std = x.std(dim = -1 , keepdim = True)\n",
    "\n",
    "        return self.alpha * (x - mean) / (std + self.eps) +  self.bias\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FeedForward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    A feedforward neural network with two linear layers and ReLU activation.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_dim:int, d_ff:int, dropout:float) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the FeedForward module.\n",
    "\n",
    "        Args:\n",
    "            model_dim (int): The dimension of the input and output.\n",
    "            d_ff (int): The dimension of the hidden layer.\n",
    "            dropout (float): The dropout probability.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # w1 and b1\n",
    "        self.linear_1 = nn.Linear(model_dim,d_ff) # First linear layer\n",
    "        self.dropout = nn.Dropout(dropout) # Dropout layer\n",
    "        # w2 and b2\n",
    "        self.linear_2 = nn.Linear(d_ff,model_dim) # Second linear layer\n",
    "    \n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        Performs the forward pass of the network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor.\n",
    "        \"\"\"\n",
    "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer : ResidualConnection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements a residual connection with dropout and layer normalization.\n",
    "\n",
    "    Args:\n",
    "        features (int): The number of features in the input tensor.\n",
    "        dropout (float): The dropout probability.\n",
    "    \"\"\"\n",
    "    def __init__(self, features: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        # Initialize dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Initialize layer normalization\n",
    "        self.norm  = LayerNormalization(features=features)\n",
    "    \n",
    "    def forward(self, x, sublayer):\n",
    "        \"\"\"\n",
    "        Performs the forward pass of the residual connection.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor.\n",
    "            sublayer (nn.Module): The sublayer to apply to the input.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor.\n",
    "        \"\"\"\n",
    "        # Apply layer normalization to the input\n",
    "        normalized_x = self.norm(x)\n",
    "        # Apply the sublayer to the normalized input\n",
    "        sublayer_output = sublayer(normalized_x)\n",
    "        # Apply dropout to the sublayer output\n",
    "        dropped_output = self.dropout(sublayer_output)\n",
    "        # Add the original input to the dropped sublayer output\n",
    "        return x + dropped_output\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Represents a single encoder block in a Transformer architecture. \n",
    "\n",
    "    Args:\n",
    "        features (int): Number of features in the input and output of the block.\n",
    "        self_attention (MultiHeadAttention): The multi-head self-attention module.\n",
    "        feed_forward (FeedForward): The feed-forward neural network.\n",
    "        dropout (float): Dropout probability.\n",
    "    \"\"\"\n",
    "    def __init__(self, features: int, self_attention:MultiHeadAttention, feed_forward: FeedForward, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize the self-attention and feed-forward modules\n",
    "        self.self_attention = self_attention\n",
    "        self.feed_forward = feed_forward\n",
    "\n",
    "        # Create a list of residual connections, one for each sub-layer\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(features,dropout) for _ in range(2)])\n",
    "    \n",
    "    def forward(self, x, src_mask):\n",
    "        \"\"\"\n",
    "        Performs a forward pass through the encoder block.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, sequence_length, features).\n",
    "            src_mask (torch.Tensor): Mask for the source sequence, used in the self-attention layer.\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, sequence_length, features).\n",
    "        \"\"\"\n",
    "        # Apply self-attention with residual connection\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention(x,x,x,src_mask))\n",
    "\n",
    "        # Apply feed-forward network with residual connection\n",
    "        x = self.residual_connections[1](x, self.feed_forward)\n",
    "\n",
    "        # Return the output of the encoder block\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder module for the Transformer model.\n",
    "\n",
    "    Args:\n",
    "        features (int): The number of features in the input sequence.\n",
    "        layers (nn.ModuleList): A list of encoder layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, features: int, layers: nn.ModuleList ) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers  # Store the encoder layers\n",
    "        self.norm = LayerNormalization(features=features) # Initialize layer normalization\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        Forward pass of the encoder.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input sequence.\n",
    "            mask (torch.Tensor): The attention mask.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The encoded sequence.\n",
    "        \"\"\"\n",
    "        for layer in self.layers:  # Iterate through each encoder layer\n",
    "            x = layer(x, mask)  # Apply the layer to the input\n",
    "            \n",
    "        return self.norm(x)  # Apply layer normalization to the output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A single decoder block in the Transformer architecture.\n",
    "\n",
    "    Args:\n",
    "        features (int): Number of features in the input and output.\n",
    "        self_attention (MultiHeadAttention): Multi-head self-attention module.\n",
    "        cross_attention (MultiHeadAttention): Multi-head cross-attention module.\n",
    "        feed_forward (FeedForward): Feed-forward neural network module.\n",
    "        dropout (float): Dropout probability.\n",
    "    \"\"\"\n",
    "    def __init__(self, features: int, self_attention: MultiHeadAttention, cross_attention:MultiHeadAttention, feed_forward: FeedForward, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention = self_attention\n",
    "        self.cross_attention = cross_attention\n",
    "        self.feed_forward = feed_forward\n",
    "        # Create 3 residual connections for self-attention, cross-attention, and feed-forward\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(features,dropout) for _ in range(3)])\n",
    "    \n",
    "\n",
    "    def forward(self, x , encoder_output, src_mask, tgt_mask):\n",
    "        \"\"\"\n",
    "        Forward pass of the decoder block.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Decoder input, shape (batch_size, seq_len, features).\n",
    "            encoder_output (torch.Tensor): Encoder output, shape (batch_size, src_seq_len, features).\n",
    "            src_mask (torch.Tensor): Source mask, shape (batch_size, src_seq_len).\n",
    "            tgt_mask (torch.Tensor): Target mask, shape (batch_size, tgt_seq_len).\n",
    "        Returns:\n",
    "            torch.Tensor: Decoder output, shape (batch_size, seq_len, features).\n",
    "        \"\"\"\n",
    "        # Apply self-attention with residual connection\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention(x,x,x, tgt_mask))\n",
    "        # Apply cross-attention with residual connection\n",
    "        x = self.residual_connections[1](x, lambda x: self.cross_attention(x,encoder_output,encoder_output,src_mask))\n",
    "        # Apply feed-forward with residual connection\n",
    "        x = self.residual_connections[2](x, self.feed_forward)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    The decoder module for the Transformer model.\n",
    "\n",
    "    Args:\n",
    "        features (int): The number of features in the decoder.\n",
    "        layers (nn.ModuleList): A list of decoder layers.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, features: int , layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize the decoder layers\n",
    "        self.layers = layers\n",
    "        # Initialize the layer normalization layer\n",
    "        self.norm = LayerNormalization(features= features)\n",
    "\n",
    "    def forward(self,x, encoder_output, src_mask, tgt_mask):\n",
    "        \"\"\"\n",
    "        Forward pass of the decoder.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The decoder input.\n",
    "            encoder_output (torch.Tensor): The encoder output.\n",
    "            src_mask (torch.Tensor): The source mask.\n",
    "            tgt_mask (torch.Tensor): The target mask.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The decoder output.\n",
    "        \"\"\"\n",
    "        # Iterate over the decoder layers\n",
    "        for layer in self.layers:\n",
    "            # Apply the current layer to the input\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "        \n",
    "        # Apply layer normalization to the output\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projection Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    This class defines a projection layer that maps a hidden state representation \n",
    "    to a vocabulary space. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_dim: int, vocab_size: int) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the ProjectionLayer.\n",
    "\n",
    "        Args:\n",
    "            model_dim (int): The dimensionality of the hidden state.\n",
    "            vocab_size (int): The size of the vocabulary.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Define a linear layer to project the hidden state to the vocabulary size\n",
    "        self.proj = nn.Linear(model_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x) -> None:\n",
    "        \"\"\"\n",
    "        Performs the forward pass of the projection layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The hidden state representation.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The projected output in the vocabulary space.\n",
    "        \"\"\"\n",
    "        # Apply the linear projection to the input\n",
    "        return self.proj(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings,src_pos: PositionalEncoding, tgt_pos: PositionalEncoding,projection_layer:ProjectionLayer):\n",
    "        \"\"\"\n",
    "        Initializes the Transformer model.\n",
    "\n",
    "        Args:\n",
    "            encoder (Encoder): The encoder module.\n",
    "            decoder (Decoder): The decoder module.\n",
    "            src_embed (InputEmbeddings): The source embedding layer.\n",
    "            tgt_embed (InputEmbeddings): The target embedding layer.\n",
    "            src_pos (PositionalEncoding): The source positional encoding layer.\n",
    "            tgt_pos (PositionalEncoding): The target positional encoding layer.\n",
    "            projection_layer (ProjectionLayer): The projection layer for the final output.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.src_pos = src_pos\n",
    "        self.tgt_pos = tgt_pos\n",
    "        self.projection_layer = projection_layer\n",
    "    \n",
    "    def encode(self, src, src_mask):\n",
    "        \"\"\"\n",
    "        Encodes the source sequence.\n",
    "\n",
    "        Args:\n",
    "            src (torch.Tensor): The source sequence.\n",
    "            src_mask (torch.Tensor): The source mask.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The encoded source sequence.\n",
    "        \"\"\"\n",
    "        src = self.src_embed(src)\n",
    "        src = self.src_pos(src)\n",
    "        return self.encoder(src, src_mask)\n",
    "    \n",
    "    def decode(self, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt: torch.Tensor, tgt_mask: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Decodes the target sequence given the encoded source sequence.\n",
    "\n",
    "        Args:\n",
    "            encoder_output (torch.Tensor): The encoded source sequence.\n",
    "            src_mask (torch.Tensor): The source mask.\n",
    "            tgt (torch.Tensor): The target sequence.\n",
    "            tgt_mask (torch.Tensor): The target mask.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The decoded target sequence.\n",
    "        \"\"\"\n",
    "        tgt = self.tgt_embed(tgt)\n",
    "        tgt = self.tgt_pos(tgt)\n",
    "        return self.decoder(tgt,encoder_output,src_mask,tgt_mask)\n",
    "    \n",
    "    def project(self,x):\n",
    "        \"\"\"\n",
    "        Projects the output of the decoder to the target vocabulary.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The output of the decoder.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The projected output.\n",
    "        \"\"\"\n",
    "        return self.projection_layer(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer(src_vocab_size: int, tgt_vocab_size:int, src_seq_len:int, tgt_seq_len: int, model_dim: int = 512 , N: int = 6, h : int = 8, dropout: float= 0.1, d_ff: int = 2048):\n",
    "    # Create Embedding Layers\n",
    "    src_embed = InputEmbeddings(model_dim,vocab_size= src_vocab_size) # Embedding layer for source language\n",
    "    tgt_embed = InputEmbeddings(model_dim,vocab_size= tgt_vocab_size) # Embedding layer for target language\n",
    "\n",
    "    # Create Positional Encoding Layers\n",
    "    src_pos = PositionalEncoding(model_dim, seq_len = src_seq_len,dropout=dropout) # Positional encoding for source language\n",
    "    tgt_pos = PositionalEncoding(model_dim, seq_len = tgt_seq_len,dropout=dropout) # Positional encoding for target language\n",
    "\n",
    "    # Create encoder blocks\n",
    "    encoder_blocks = []\n",
    "    for _ in range(N): # Create N encoder blocks\n",
    "        encoder_self_attention_block = MultiHeadAttention(model_dim,h,dropout) # Multi-head self-attention block for encoder\n",
    "        feed_forward_block = FeedForward(model_dim,d_ff,dropout) # Feed-forward network for encoder\n",
    "        encoder_block = EncoderBlock(features=model_dim,self_attention=encoder_self_attention_block,feed_forward=feed_forward_block,dropout=dropout) # Combine attention and feedforward for an encoder block\n",
    "        encoder_blocks.append(encoder_block)\n",
    "\n",
    "    # Create decoder blocks\n",
    "    decoder_blocks = []\n",
    "    for _ in range(N): # Create N decoder blocks\n",
    "        decoder_self_attention_block = MultiHeadAttention(model_dim,h,dropout) # Multi-head self-attention block for decoder\n",
    "        decoder_cross_attention_block = MultiHeadAttention(model_dim,h, dropout) # Multi-head cross-attention block for decoder\n",
    "\n",
    "        feed_forward_block = FeedForward(model_dim,d_ff,dropout) # Feed-forward network for decoder\n",
    "        decoder_block = DecoderBlock(features=model_dim,self_attention=decoder_self_attention_block,cross_attention=decoder_cross_attention_block,feed_forward=feed_forward_block,dropout=dropout) # Combine attention and feedforward for a decoder block\n",
    "        decoder_blocks.append(decoder_block)\n",
    "        \n",
    "    \n",
    "    # Create the encoder and decoder block\n",
    "    encoder = Encoder(model_dim, nn.ModuleList(encoder_blocks)) # Combine all encoder blocks\n",
    "    decoder = Decoder(model_dim, nn.ModuleList(decoder_blocks)) # Combine all decoder blocks\n",
    "\n",
    "    # Create the projection layer\n",
    "    projection_layer = ProjectionLayer(model_dim=model_dim,vocab_size=tgt_vocab_size) # Layer to project decoder output to target language vocabulary\n",
    "\n",
    "    # Create the Transformer\n",
    "    transformer = Transformer(encoder,decoder,src_embed,tgt_embed,src_pos,tgt_pos,projection_layer) # Combine all components into the transformer\n",
    "\n",
    "    # Intialize the parameters , rather than starting randomly\n",
    "    for p in transformer.parameters(): \n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform(p) # Initialize parameters using Xavier uniform initialization\n",
    "    return transformer\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Transformer NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets tokenizers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dipanjanchowdhury/Labs/OpenCode/ML4Interviews/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: This is a test with some inner quotes.\n",
      "Text 2: This is another test with more inner quotes.\n",
      "Text 3: This has single and double quotes.\n"
     ]
    }
   ],
   "source": [
    "def remove_inner_quotes(text):\n",
    "  \"\"\"\n",
    "  Removes inner quotes (both single and double) if they form a pair.\n",
    "\n",
    "  Args:\n",
    "    text: The input string with potential inner quotes.\n",
    "\n",
    "  Returns:\n",
    "    The string with inner quotes removed, if present.\n",
    "  \"\"\"\n",
    "  quote_type = None\n",
    "  result = []\n",
    "  for char in text:\n",
    "    if char in (\"'\", '\"'):\n",
    "      if quote_type is None:\n",
    "        quote_type = char\n",
    "      elif char == quote_type:\n",
    "        quote_type = None\n",
    "      else:\n",
    "        result.append(char)\n",
    "    else:\n",
    "      result.append(char)\n",
    "  return \"\".join(result)\n",
    "\n",
    "# Example usage:\n",
    "text1 = \"This is a test 'with' some inner quotes.\"\n",
    "text2 = 'This is another test \"with\" more inner quotes.'\n",
    "text3 = \"This has 'single' and \\\"double\\\" quotes.\"\n",
    "\n",
    "print(f\"Text 1: {remove_inner_quotes(text1)}\")\n",
    "print(f\"Text 2: {remove_inner_quotes(text2)}\")\n",
    "print(f\"Text 3: {remove_inner_quotes(text3)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_trailing_newline(text):\n",
    "  \"\"\"\n",
    "  Removes trailing newline characters (\\n) from a string.\n",
    "\n",
    "  Args:\n",
    "    text: The input string.\n",
    "  Returns:\n",
    "    The string with trailing newlines removed.\n",
    "  \"\"\"\n",
    "  return text.rstrip('\\n').strip()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def preprocess(text: str):\n",
    "  \"\"\"\n",
    "  Preprocesses a string by removing inner quotes and trailing newlines.\n",
    "\n",
    "  Args:\n",
    "    text: The input string.\n",
    "\n",
    "  Returns:\n",
    "    The preprocessed string.\n",
    "  \"\"\"\n",
    "  text = remove_inner_quotes(text) # Remove inner quotes\n",
    "  text = remove_trailing_newline(text) # Remove trailing newlines\n",
    "  return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langdetect\n",
    "\n",
    "def detect_text_language(text):\n",
    "  \"\"\"\n",
    "  Detects the language of the given text using the langdetect library.\n",
    "\n",
    "  Args:\n",
    "    text: The text string to analyze.\n",
    "\n",
    "  Returns:\n",
    "    A string representing the detected language code (e.g., 'en', 'fr', 'es')\n",
    "    or 'Unknown' if no language could be confidently detected.\n",
    "  \"\"\"\n",
    "  try:\n",
    "    return langdetect.detect(text)\n",
    "  except langdetect.LangDetectException:\n",
    "    return 'Unknown'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [06:18<00:00, 264.52it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "directory = Path.cwd().parents[1]\n",
    "data_dir = os.path.join(directory,\"data\")\n",
    "filepath = os.path.join(data_dir,\"x00.txt\")\n",
    "\n",
    "# with open(filepath,\"r\",encoding=\"utf-8\") as file:\n",
    "#     data = file.readlines()\n",
    "\n",
    "\n",
    "# def create_dataset(data):\n",
    "#     dataset = []\n",
    "    \n",
    "#     for record in tqdm(data):\n",
    "#         fields = record.split(\"\\t\")\n",
    "#         src_text = preprocess(fields[2])\n",
    "#         tgt_text = preprocess(fields[1])\n",
    "        \n",
    "#         dataset.append({\n",
    "#             \"src\": src_text,\n",
    "#             \"tgt\": tgt_text,\n",
    "#             \"src_lang\": detect_text_language(src_text),\n",
    "#             \"tgt_lang\": detect_text_language(tgt_text),\n",
    "#             \"src_len\": len(src_text),\n",
    "#             \"tgt_len\":  len(tgt_text)\n",
    "#         })\n",
    "    \n",
    "#     return dataset\n",
    "\n",
    "# dataset = create_dataset(data)\n",
    "\n",
    "# with open(os.path.join(data_dir,'translation_dataset_01.json'), 'w',encoding=\"utf-8\") as json_file:\n",
    "#     json.dump(dataset, json_file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.join(data_dir,\"translation_dataset_01.json\")\n",
    "\n",
    "dataset = json.load(open(dataset_path,encoding=\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:00<00:00, 2461287.12it/s]\n"
     ]
    }
   ],
   "source": [
    "# Function to filter records by src_lang and tgt_lang\n",
    "def filter_records(data, src_lang, tgt_lang):\n",
    "    return [record for record in tqdm(data) if record['src_lang'] == src_lang and record['tgt_lang'] == tgt_lang]\n",
    "\n",
    "\n",
    "# Call the function to filter records where src_lang is 'en' and tgt_lang is 'bn'\n",
    "filtered_data = filter_records(dataset, src_lang='en', tgt_lang='bn')\n",
    "\n",
    "# Print the filtered data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_sentences = [ data.get(\"tgt\") for data in filtered_data]\n",
    "src_sentences = [ data.get(\"src\") for data in filtered_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders, processors\n",
    "# tokenizer_path = os.path(data_dir,'bengali_bpe_tokenizer.json')\n",
    "\n",
    "def create_or_load_tokenizer(sentences: str , tokenizer_path: os.path, vocab_size:int =30000 , min_frequency: int =2):\n",
    "    # Check if the tokenizer file already exists\n",
    "    if os.path.exists(tokenizer_path):\n",
    "        # Load the existing tokenizer\n",
    "        tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "        print(f\"Tokenizer loaded from {tokenizer_path}\")\n",
    "    else:\n",
    "        # Initialize a BPE tokenizer\n",
    "        tokenizer = Tokenizer(models.BPE())\n",
    "\n",
    "        # Define a pre-tokenizer (to split on whitespace and punctuation)\n",
    "        tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "        # Define the BPE trainer\n",
    "        trainer = trainers.BpeTrainer(vocab_size=vocab_size, min_frequency=min_frequency, \n",
    "                                      special_tokens=[\"[PAD]\", \"[UNK]\", \"[SOS]\", \"[EOS]\"])\n",
    "\n",
    "        # Train the tokenizer on your Bengali dataset\n",
    "        tokenizer.train_from_iterator(sentences, trainer)\n",
    "\n",
    "        # Save the tokenizer for future use\n",
    "        tokenizer.save(tokenizer_path)\n",
    "        print(f\"Tokenizer trained and saved to {tokenizer_path}\")\n",
    "\n",
    "    return tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded from /Users/dipanjanchowdhury/Labs/OpenCode/ML4Interviews/data/en_src_bpe_tokenizer_01.json\n",
      "Tokenizer loaded from /Users/dipanjanchowdhury/Labs/OpenCode/ML4Interviews/data/bn_tgt_bpe_tokenizer_01.json\n"
     ]
    }
   ],
   "source": [
    "tokenizer_src = create_or_load_tokenizer(sentences=src_sentences, tokenizer_path= os.path.join(data_dir,\"en_src_bpe_tokenizer_01.json\"))\n",
    "tokenizer_tgt = create_or_load_tokenizer(sentences=tgt_sentences, tokenizer_path= os.path.join(data_dir,\"bn_tgt_bpe_tokenizer_01.json\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def casual_mask(size):\n",
    "    \"\"\"\n",
    "    Creates a casual mask for a sequence of length `size`.\n",
    "\n",
    "    Args:\n",
    "        size: The length of the sequence.\n",
    "\n",
    "    Returns:\n",
    "        A PyTorch tensor of shape (1, size, size) where:\n",
    "        - 1s represent valid connections (i.e., elements can see themselves and elements before them)\n",
    "        - 0s represent invalid connections (i.e., elements cannot see elements after them)\n",
    "\n",
    "    Example:\n",
    "        >>> casual_mask(5)\n",
    "        tensor([[[ True,  True,  True,  True,  True],\n",
    "                [False,  True,  True,  True,  True],\n",
    "                [False, False,  True,  True,  True],\n",
    "                [False, False, False,  True,  True],\n",
    "                [False, False, False, False,  True]]])\n",
    "    \"\"\"\n",
    "    mask  = torch.triu(torch.ones((1,size, size)),diagonal=1).type(torch.int)\n",
    "    return mask == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "# It should contain both src and tgt sentences,\n",
    "class MultilingualDataset(Dataset):\n",
    "    def __init__(self, src_sentences,tgt_sentences, tokenizer_src, tokenizer_tgt, seq_len, dtype = torch.int64) -> None:\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.dtype = dtype\n",
    "\n",
    "        self.src_sentences = src_sentences\n",
    "        self.tgt_sentences = tgt_sentences\n",
    "        self.tokenizer_src = tokenizer_src\n",
    "        self.tokenizer_tgt = tokenizer_tgt\n",
    "\n",
    "        self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=self.dtype)\n",
    "        self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=self.dtype)\n",
    "        self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=self.dtype)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(src_sentences)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        src_text = self.src_sentences[index]\n",
    "        tgt_text = self.tgt_sentences[index]\n",
    "\n",
    "        # Transform the text into tokens\n",
    "        encoded_input_tokens = self.tokenizer_src.encode(src_text).ids\n",
    "        decoded_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n",
    "       \n",
    "        # Add [SOS],[EOS] and padding to each sentence\n",
    "        enc_num_padding_tokens = self.seq_len - len(encoded_input_tokens) - 2 # for [SOS] and [EOS] token\n",
    "\n",
    "        # For decoder add just [SOS]\n",
    "        dec_num_padding_tokens = self.seq_len - len(decoded_input_tokens) - 1\n",
    "\n",
    "        # Make sure the number of padding is not negative.\n",
    "        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n",
    "            raise ValueError(f\"sentence is more than seq_len:{seq_length}\")\n",
    "        \n",
    "\n",
    "        # Add [SOS],[EOS] token\n",
    "        # Concatenate the [SOS], encoded tokens, [EOS] and padding tokens for the encoder input\n",
    "        encoder_input = torch.cat(\n",
    "            [\n",
    "                self.sos_token,  # Start of sentence token\n",
    "                torch.tensor(encoded_input_tokens,dtype=self.dtype),  # Encoded source sentence tokens\n",
    "                self.eos_token,  # End of sentence token\n",
    "                torch.tensor([self.pad_token]* enc_num_padding_tokens,dtype=self.dtype)  # Padding tokens to reach the desired sequence length\n",
    "            ],\n",
    "            dim= 0 \n",
    "        )\n",
    "        # Add only [S0S] token\n",
    "        decoder_input = torch.cat(\n",
    "            [\n",
    "                self.sos_token,\n",
    "                torch.tensor(decoded_input_tokens,dtype=self.dtype),\n",
    "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype= self.dtype)\n",
    "            ],\n",
    "            dim = 0\n",
    "        )\n",
    "\n",
    "        label = torch.cat(\n",
    "            [\n",
    "                torch.tensor(decoded_input_tokens,dtype=self.dtype),\n",
    "                self.eos_token,\n",
    "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=self.dtype)\n",
    "\n",
    "            ],\n",
    "            dim = 0,\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"encoder_input\": encoder_input,\n",
    "            \"decoder_input\" :  decoder_input,\n",
    "            \"encoder_mask\" : (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(),\n",
    "            \"decoder_mask\" : (decoder_input != self.pad_token).unsqueeze(0).int() & casual_mask(decoder_input.size(0)), # (1, seq_len) & (1, seq_len, seq_len),\n",
    "            \"label\": label,  # (seq_len)\n",
    "            \"src_text\": src_text,\n",
    "            \"tgt_text\" : tgt_text\n",
    "            \n",
    "        }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = MultilingualDataset(src_sentences=src_sentences,tgt_sentences=tgt_sentences,tokenizer_src=tokenizer_src,tokenizer_tgt=tokenizer_tgt,seq_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'encoder_input': tensor([    2,  1387,  2387,  2273,  3192,  1417,  1515,  1616,  1377,  1345,\n",
       "         26048,  1355,  2273,  6177,    17,     3,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]),\n",
       " 'decoder_input': tensor([    2,  5719, 13178,  1431,  8971,  7467,  1335,  4872, 15856,  1990,\n",
       "         12349,  1309,  1286,   530,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]),\n",
       " 'encoder_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0]]], dtype=torch.int32),\n",
       " 'decoder_mask': tensor([[[1, 0, 0,  ..., 0, 0, 0],\n",
       "          [1, 1, 0,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0]]], dtype=torch.int32),\n",
       " 'label': tensor([ 5719, 13178,  1431,  8971,  7467,  1335,  4872, 15856,  1990, 12349,\n",
       "          1309,  1286,   530,     3,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]),\n",
       " 'src_text': 'The six official languages are also used for the dissemination of official documents.',\n",
       " 'tgt_text': 'অফিসিয়াল ডকুমেন্টগুলো প্রচারের জন্যও এই ছয়টি অফিশিয়াল ভাষা ব্যাবহার করা হয়।'}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[122]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "def test_train_split(dataset,prob : float = 0.9):\n",
    "    # Keep 90% for training, 10% for validation\n",
    "    train_ds_size = int(prob * len(dataset))\n",
    "    val_ds_size = len(dataset) - train_ds_size\n",
    "    train_ds, val_ds = random_split(dataset, [train_ds_size, val_ds_size])\n",
    "    return train_ds, val_ds\n",
    "\n",
    "def get_dataset(dataset):\n",
    "\n",
    "    \n",
    "    tokenizer_src = create_or_load_tokenizer(sentences=src_sentences, tokenizer_path= os.path.join(data_dir,\"en_src_bpe_tokenizer_01.json\"))\n",
    "    tokenizer_tgt = create_or_load_tokenizer(sentences=tgt_sentences, tokenizer_path= os.path.join(data_dir,\"bn_tgt_bpe_tokenizer_01.json\"))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
